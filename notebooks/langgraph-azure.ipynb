{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10244554,"sourceType":"datasetVersion","datasetId":6335700}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install elasticsearch\n!pip install faiss-cpu sentence-transformers\n!pip install langchain\n!pip install -qU langchain-openai\n!pip install langgraph","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:04:39.621923Z","iopub.execute_input":"2024-12-19T12:04:39.622127Z","iopub.status.idle":"2024-12-19T12:05:06.535421Z","shell.execute_reply.started":"2024-12-19T12:04:39.622106Z","shell.execute_reply":"2024-12-19T12:05:06.534259Z"}},"outputs":[{"name":"stdout","text":"Collecting elasticsearch\n  Downloading elasticsearch-8.17.0-py3-none-any.whl.metadata (8.8 kB)\nCollecting elastic-transport<9,>=8.15.1 (from elasticsearch)\n  Downloading elastic_transport-8.15.1-py3-none-any.whl.metadata (3.7 kB)\nRequirement already satisfied: urllib3<3,>=1.26.2 in /usr/local/lib/python3.10/dist-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (2.2.3)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (2024.8.30)\nDownloading elasticsearch-8.17.0-py3-none-any.whl (571 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.2/571.2 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading elastic_transport-8.15.1-py3-none-any.whl (64 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: elastic-transport, elasticsearch\nSuccessfully installed elastic-transport-8.15.1 elasticsearch-8.17.0\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\nCollecting sentence-transformers\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-cpu, sentence-transformers\nSuccessfully installed faiss-cpu-1.9.0.post1 sentence-transformers-3.3.1\nCollecting langchain\n  Downloading langchain-0.3.13-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\nCollecting langchain-core<0.4.0,>=0.3.26 (from langchain)\n  Downloading langchain_core-0.3.27-py3-none-any.whl.metadata (6.3 kB)\nCollecting langchain-text-splitters<0.4.0,>=0.3.3 (from langchain)\n  Downloading langchain_text_splitters-0.3.4-py3-none-any.whl.metadata (2.3 kB)\nCollecting langsmith<0.3,>=0.1.17 (from langchain)\n  Downloading langsmith-0.2.4-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\nCollecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.26->langchain)\n  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.26->langchain) (24.1)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.26->langchain) (4.12.2)\nCollecting httpx<1,>=0.23.0 (from langsmith<0.3,>=0.1.17->langchain)\n  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\nCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.3,>=0.1.17->langchain)\n  Downloading orjson-3.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.3,>=0.1.17->langchain)\n  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (3.7.1)\nCollecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain)\n  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\nCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain)\n  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.26->langchain) (3.0.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.2)\nDownloading langchain-0.3.13-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading langchain_core-0.3.27-py3-none-any.whl (411 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.5/411.5 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.3.4-py3-none-any.whl (27 kB)\nDownloading langsmith-0.2.4-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\nDownloading orjson-3.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.3/131.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: orjson, jsonpatch, h11, requests-toolbelt, httpcore, httpx, langsmith, langchain-core, langchain-text-splitters, langchain\nSuccessfully installed h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 jsonpatch-1.33 langchain-0.3.13 langchain-core-0.3.27 langchain-text-splitters-0.3.4 langsmith-0.2.4 orjson-3.10.12 requests-toolbelt-1.0.0\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.3/454.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.0/345.0 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting langgraph\n  Downloading langgraph-0.2.60-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43 in /usr/local/lib/python3.10/dist-packages (from langgraph) (0.3.27)\nCollecting langgraph-checkpoint<3.0.0,>=2.0.4 (from langgraph)\n  Downloading langgraph_checkpoint-2.0.9-py3-none-any.whl.metadata (4.6 kB)\nCollecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n  Downloading langgraph_sdk-0.1.48-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (6.0.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (1.33)\nRequirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (0.2.4)\nRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (24.1)\nRequirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.9.2)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (9.0.0)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (4.12.2)\nCollecting msgpack<2.0.0,>=1.1.0 (from langgraph-checkpoint<3.0.0,>=2.0.4->langgraph)\n  Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\nRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.10/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\nRequirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.10/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.12)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.7.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (2024.8.30)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.7)\nRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (3.0.0)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.32.3)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (1.0.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.23.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.2.3)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.2.2)\nDownloading langgraph-0.2.60-py3-none-any.whl (135 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.7/135.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langgraph_checkpoint-2.0.9-py3-none-any.whl (37 kB)\nDownloading langgraph_sdk-0.1.48-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.0/378.0 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: msgpack, langgraph-sdk, langgraph-checkpoint, langgraph\n  Attempting uninstall: msgpack\n    Found existing installation: msgpack 1.0.8\n    Uninstalling msgpack-1.0.8:\n      Successfully uninstalled msgpack-1.0.8\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndistributed 2024.8.0 requires dask==2024.8.0, but you have dask 2024.12.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed langgraph-0.2.60 langgraph-checkpoint-2.0.9 langgraph-sdk-0.1.48 msgpack-1.1.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Load Elastic Search","metadata":{}},{"cell_type":"code","source":"from elasticsearch import Elasticsearch, helpers\nfrom kaggle_secrets import UserSecretsClient\nimport pandas as pd\n\n# Initialize Elasticsearch client\n#es = Elasticsearch(\"http://localhost:9200/\")\n\nuser_secrets = UserSecretsClient() \n# Initialize Elasticsearch client\nCLOUD_ID = user_secrets.get_secret(\"CLOUD_ID\")\nELASTIC_PASSWORD = user_secrets.get_secret(\"ELASTIC_PASSWORD\")\n\nes = Elasticsearch(\n    cloud_id=CLOUD_ID,\n    basic_auth=(\"elastic\", ELASTIC_PASSWORD)\n)\nif es.ping():\n    print(\"Connected to Elasticsearch!\")\n    try:\n        response = es.indices.delete(index=\"emails\")\n        print(f\"Successfully deleted index: emails\")\n    except Exception as e:\n        print(f\"Error deleting index/Index not found: {e}\")\nelse:\n    print(\"Connection failed.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:06:46.018377Z","iopub.execute_input":"2024-12-19T12:06:46.018718Z","iopub.status.idle":"2024-12-19T12:06:47.305929Z","shell.execute_reply.started":"2024-12-19T12:06:46.018694Z","shell.execute_reply":"2024-12-19T12:06:47.305295Z"}},"outputs":[{"name":"stdout","text":"Connected to Elasticsearch!\nError deleting index/Index not found: NotFoundError(404, 'index_not_found_exception', 'no such index [emails]', emails, index_or_alias)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Load FAISS","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n\nembedding_model = SentenceTransformer(\"dunzhang/stella_en_1.5B_v5\", trust_remote_code=True, device=\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:06:50.186352Z","iopub.execute_input":"2024-12-19T12:06:50.186789Z","iopub.status.idle":"2024-12-19T12:09:45.279613Z","shell.execute_reply.started":"2024-12-19T12:06:50.186761Z","shell.execute_reply":"2024-12-19T12:09:45.278680Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3095ae8e2384ee79f9ca5c2e65c41f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/397 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4d6f04a17224e1e8184f2338967be02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/169k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc46e8996e784068bc8b6d3a65e02370"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/51.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9699dfd514d6429b88f85f29f7db2fbc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/844 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3309b8a5e566405097ccf37af12b837b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modeling_qwen.py:   0%|          | 0.00/65.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2320d88c09d540a0963a9722888639c1"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/dunzhang/stella_en_1.5B_v5:\n- modeling_qwen.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/6.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a35b5ccb0734b9bb1ddef54cce80510"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.31k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d76996b979f04fab81c16f105cc2b15b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenization_qwen.py:   0%|          | 0.00/10.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45b5b581b83d4149889c3ab9ae7fe1f5"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/dunzhang/stella_en_1.5B_v5:\n- tokenization_qwen.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1631ff6e0d564fa6a908b651d6988253"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"322d38b3447043d3b235bc4ba02726ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b123f298068149c29192ffb936c8a9b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/80.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d61ab7d254b44c0ba2cb88f60db359a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/370 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ab7427e88f9406eb3adc38e225f8242"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/289 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5daa86b08524467e865ad49ef1b77d5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"2_Dense_1024/config.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd17918610c746048ec8e21e9b59c8e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/6.30M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe450343cc5544119ecc56d241adee45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/6.30M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e598dddfa2914987bec554fe4bf01921"}},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# Semantic Hybrid Search","metadata":{}},{"cell_type":"code","source":"import faiss\nimport json\nfrom elasticsearch.helpers import bulk\nfrom difflib import SequenceMatcher\nfrom typing import List, Dict, Any\nimport os\nimport re\n\nclass SemanticHybridSearch:\n    \"\"\"\n    A class that combines Elasticsearch for keyword-based lexical searches and Faiss for semantic searches.\n\n    This class provides methods to load and search both Elasticsearch and Faiss indices,\n    as well as a hybrid search method that combines results from both search types.\n\n    Attributes:\n        data (list): The dataset used for searching.\n        es_client (Elasticsearch): Elasticsearch client for performing lexical searches.\n        embedding_model: Model used for encoding queries into embeddings.\n        vector_index (faiss.Index): Faiss index for semantic searches.\n        elastic_index_name (str): Name of the Elasticsearch index.\n    \"\"\"\n\n    def __init__(\n        self,\n        es_client,\n        embedding_model,\n        data: list,\n        elastic_index_path: str,\n        vector_index_path: str,\n    ):\n        \"\"\"\n        Initialize the SemanticHybridSearch class.\n\n        Args:\n            es_client (Elasticsearch): Elasticsearch client.\n            embedding_model: Model for encoding queries into embeddings.\n            data (list): Dataset used for searching.\n            elastic_index_path (str): Path to the Elasticsearch index file.\n            vector_index_path (str): Path to the Faiss vector index file.\n        \"\"\"\n        self.data = data\n        self.es_client = es_client\n        self.embedding_model = embedding_model\n        self.vector_index = self.load_vector_index(vector_index_path)\n        self.elastic_index = self.load_elastic_index(elastic_index_path)\n\n        self.elastic_index_name = \"\"\n\n    def load_elastic_index(self, elastic_index_path: str):\n        \"\"\"\n        Load the Elasticsearch index from a file.\n\n        Args:\n            elastic_index_path (str): Path to the Elasticsearch index file.\n        \"\"\"\n        with open(elastic_index_path) as f:\n            documents = json.load(f)\n            self.elastic_index_name = os.path.basename(elastic_index_path)\n            print(f\"Loading Index {self.elastic_index_name}\")\n\n            actions = [\n                {\n                    \"_index\": self.elastic_index_name,\n                    \"_id\": doc[\"_id\"],\n                    \"_source\": doc[\"_source\"],\n                }\n                for doc in documents\n            ]\n            bulk(self.es_client, actions)\n\n    def load_vector_index(self, vector_index_path: str):\n        \"\"\"\n        Load the Faiss vector index from a file.\n\n        Args:\n            vector_index_path (str): Path to the Faiss vector index file.\n\n        Returns:\n            faiss.Index: Loaded Faiss index.\n        \"\"\"\n        print(f\"Loading Index {os.path.basename(vector_index_path)}\")\n        index = faiss.read_index(vector_index_path)\n        return index\n\n    def elastic_search(self, query: dict, top_k: int = 3) -> list:\n        \"\"\"\n        Perform a keyword-based search using Elasticsearch.\n\n        Args:\n            query (dict): Elasticsearch query.\n            top_k (int): Number of top results to return. Defaults to 3.\n\n        Returns:\n            list: Top k search results.\n        \"\"\"\n        try:\n            results = self.es_client.search(index=self.elastic_index_name, body=query)\n        except Exception as e:\n            print(\"Invalid Query\", query, e)\n            return []\n        return [result[\"_source\"] for result in results[\"hits\"][\"hits\"][:top_k]]\n\n    def semantic_search(self, query: str, top_k: int = 3) -> list:\n        \"\"\"\n        Perform a semantic search using Faiss.\n\n        Args:\n            query (str): Search query.\n            top_k (int): Number of top results to return. Defaults to 3.\n\n        Returns:\n            list: Top k search results.\n        \"\"\"\n        embedding = self.embedding_model.encode([query]).astype(\"float32\")\n        distances, idx = self.vector_index.search(embedding, top_k)\n        results = [self.data[i] for i in idx[0]]\n\n        return results\n\n    def hybrid_search(\n        self,\n        elastic_query: dict,\n        semantic_query: str,\n        top_k: tuple = (3, 3),\n        clean_overlap: bool = True,\n    ) -> list:\n        \"\"\"\n        Perform a hybrid search combining results from Elasticsearch and Faiss.\n\n        Args:\n            elastic_query (dict): Elasticsearch query for lexical search.\n            semantic_query (str): Query string for semantic search.\n            top_k (tuple): Tuple containing the number of top results to return for (elastic, semantic) searches. Defaults to 3.\n            clean_overlap (bool): Whether to remove overlap in email threads results. Defaults to True.\n\n        Returns:\n            list: Combined and deduplicated search results.\n        \"\"\"\n        elastic_results = self.elastic_search(elastic_query, top_k[0])\n        semantic_results = self.semantic_search(semantic_query, top_k[1])\n\n        hybrid_concat = pd.concat(\n            [pd.DataFrame(elastic_results), pd.DataFrame(semantic_results)],\n            ignore_index=True,\n        ).drop_duplicates()\n        hybrid_results = hybrid_concat.to_dict(orient=\"records\")\n\n        if clean_overlap:\n            return self._extract_unique_content(hybrid_results)\n        return hybrid_results\n\n    def _clean_text(self, text: str) -> str:\n        \"\"\"\n        Remove extra whitespace and newlines from the given text.\n\n        Args:\n            text (str): The input text to be cleaned.\n\n        Returns:\n            str: The cleaned text with extra whitespace removed.\n        \"\"\"\n        return re.sub(r\"\\s+\", \" \", text).strip()\n\n    def _find_overlap(self, text1: str, text2: str) -> str:\n        \"\"\"\n        Find the longest common substring between two texts.\n\n        Args:\n            text1 (str): The first text to compare.\n            text2 (str): The second text to compare.\n\n        Returns:\n            str: The longest common substring, or an empty string if no overlap is found.\n        \"\"\"\n        matcher = SequenceMatcher(None, text1, text2)\n        match = matcher.find_longest_match(0, len(text1), 0, len(text2))\n        return text1[match.a : match.a + match.size] if match.size > 0 else \"\"\n\n    def _extract_unique_content(\n        self, emails: List[Dict[str, Any]]\n    ) -> List[Dict[str, Any]]:\n        \"\"\"\n        Extract unique content from a list of email dictionaries by removing overlapping text.\n\n        This function processes a list of email dictionaries, removing any overlapping content\n        between emails to reduce redundancy. It preserves the original email structure and\n        metadata while modifying only the 'Mail_Body' field.\n\n        Args:\n            emails (List[Dict[str, Any]]): A list of dictionaries, each representing an email\n            keys for 'Origin', 'Subject', 'To', 'From', 'Cc', 'Bcc', 'Date', 'Attachment_Count',\n            and 'Mail_Body'.\n\n        Returns:\n            List[Dict[str, Any]]: A list of dictionaries with the same structure as the input,\n            but with overlapping content removed from the 'Mail_Body' field.\n\n        Note:\n            This function assumes that emails are ordered chronologically, with newer emails\n            appearing later in the list.\n        \"\"\"\n        unique_contents = []\n\n        for i, email in enumerate(emails):\n            current_email = self._clean_text(email[\"Mail_Body\"])\n            unique_content = current_email\n\n            for j in range(i):\n                previous_email = self._clean_text(emails[j][\"Mail_Body\"])\n                overlap = self._find_overlap(previous_email, current_email)\n\n                if len(overlap) > 10:\n                    unique_content = unique_content.replace(overlap, \"\").strip()\n\n            unique_contents.append(\n                {\n                    \"Origin\": email[\"Origin\"],\n                    \"Subject\": email[\"Subject\"],\n                    \"To\": email[\"To\"],\n                    \"From\": email[\"From\"],\n                    \"Cc\": email[\"Cc\"],\n                    \"Bcc\": email[\"Bcc\"],\n                    \"Date\": email[\"Date\"],\n                    \"Attachment_Count\": email[\"Attachment_Count\"],\n                    \"Mail_Body\": unique_content,\n                }\n            )\n\n        return unique_contents","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:20:07.410984Z","iopub.execute_input":"2024-12-19T12:20:07.411801Z","iopub.status.idle":"2024-12-19T12:20:07.452411Z","shell.execute_reply.started":"2024-12-19T12:20:07.411767Z","shell.execute_reply":"2024-12-19T12:20:07.451728Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"emails = pd.read_csv(\"/kaggle/input/llm-testing/set_tracked.csv\").fillna(\"\") # Index cannot parse nan\nemail_dict = emails.to_dict(orient='records')\nemail_dict[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:20:11.381594Z","iopub.execute_input":"2024-12-19T12:20:11.381912Z","iopub.status.idle":"2024-12-19T12:20:11.730755Z","shell.execute_reply.started":"2024-12-19T12:20:11.381889Z","shell.execute_reply":"2024-12-19T12:20:11.730071Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'Origin': '11-01-24 report simran.eml',\n 'Subject': '11-01-24 report simran',\n 'To': 'pushpamupadhyay.bavdhan@shapoorji.com, Pushpamupadhyay Bavdhan <pushpamupadhyay.bavdhan@shapoorji.com>',\n 'From': 'Simran <simransinha6426@gmail.com>',\n 'Cc': '',\n 'Bcc': '',\n 'Date': 'Thu, 11 Jan 2024 18:55:15 +0530',\n 'Attachment_Count': 1,\n 'Mail_Body': 'CAUTION: This email has originated outside of Shapoorji Pallonji. Do not click on any links or open any attachments, unless you recognize the sender and know the content is safe.',\n 'UID': 0}"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"search_tool = SemanticHybridSearch(es, embedding_model, email_dict, \"/kaggle/input/llm-testing/index_tracked_elastic.json\", \"/kaggle/input/llm-testing/index_tracked_semantic.index\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:20:13.667312Z","iopub.execute_input":"2024-12-19T12:20:13.667594Z","iopub.status.idle":"2024-12-19T12:20:13.985698Z","shell.execute_reply.started":"2024-12-19T12:20:13.667572Z","shell.execute_reply":"2024-12-19T12:20:13.984835Z"}},"outputs":[{"name":"stdout","text":"Loading Index index_tracked_semantic.index\nLoading Index index_tracked_elastic.json\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Sanity Check Search Tool","metadata":{}},{"cell_type":"code","source":"elastic_search_query = {\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n        {\n          \"match\": {\n            \"To\": {\n              \"query\": \"Pushpam\",\n              \"fuzziness\": \"AUTO\"\n            }\n          },\n        }\n      ]\n    }\n  }\n} # Fuzzy search\nsearch_tool.elastic_search(elastic_search_query, 3)\nsearch_tool.hybrid_search(elastic_search_query, \"I want to buy property\", (3,3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:20:17.419447Z","iopub.execute_input":"2024-12-19T12:20:17.419765Z","iopub.status.idle":"2024-12-19T12:20:18.953508Z","shell.execute_reply.started":"2024-12-19T12:20:17.419740Z","shell.execute_reply":"2024-12-19T12:20:18.952772Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"374a697e3b5a4c539f5bafc237f30e17"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"[{'Origin': 'Fwd_ Spreadsheet shared with you_ ‘Vanaha x Akkhilesh Mane Call Sheet’.eml',\n  'Subject': 'Fwd: Spreadsheet shared with you: ‘Vanaha x Akkhilesh Mane Call Sheet’',\n  'To': 'Pushpam Upadhyay <pushpamupadhyay91@gmail.com>',\n  'From': 'Pushpamupadhyay Bavdhan <pushpamupadhyay.bavdhan@shapoorji.com>',\n  'Cc': '',\n  'Bcc': '',\n  'Date': 'Fri, 10 May 2024 06:14:31 +0000',\n  'Attachment_Count': 0,\n  'Mail_Body': 'Get Outlook for AndroidFrom: Akhilesh Mane (via Google Sheets) <drive-shares-dm-noreply@google.com>Sent: Thursday, May 2, 2024 1:00:45 amTo: Pushpamupadhyay Bavdhan <pushpamupadhyay.bavdhan@shapoorji.com>Subject: Spreadsheet shared with you: ‘Vanaha x Akkhilesh Mane Call Sheet’ CAUTION: This email has originated outside of Shapoorji Pallonji. Do not click on any links or open any attachments, unless you recognize the sender and know the content is safe.Akhilesh Mane shared a spreadsheetAkhilesh Mane (am@23estates.com) added you as an editor. Verify your email address to make edits securely to this spreadsheet. You will need to verify your email address every 7 days. Learn moreVanaha x Akkhilesh Mane Call SheetOpenUse is subject to the Google Privacy Policy.Google LLC, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USAYou have received this email because am@23estates.com shared a spreadsheet with you from Google Sheets. Delete visitor sessionGoogle Workspace'},\n {'Origin': 'untitled mail (1).eml',\n  'Subject': 'No Subject Specified',\n  'To': 'Pushpam Upadhyay <pushpamupadhyay91@gmail.com>',\n  'From': 'pushpamupadhyay.bavdhan@shapoorji.com',\n  'Cc': '',\n  'Bcc': '',\n  'Date': 'Sat, 04 May 2024 10:00:58 +0000',\n  'Attachment_Count': 1,\n  'Mail_Body': ''},\n {'Origin': 'untitled mail.eml',\n  'Subject': 'No Subject Specified',\n  'To': 'Pushpam Upadhyay <pushpamupadhyay91@gmail.com>',\n  'From': 'pushpamupadhyay.bavdhan@shapoorji.com',\n  'Cc': '',\n  'Bcc': '',\n  'Date': 'Thu, 07 Mar 2024 11:51:31 +0000',\n  'Attachment_Count': 1,\n  'Mail_Body': ''},\n {'Origin': 'Fwd_ Proposal for land.eml',\n  'Subject': 'Fwd: Proposal for land',\n  'To': 'Pushpamupadhyay Bavdhan <pushpamupadhyay.bavdhan@shapoorji.com>',\n  'From': 'amol yadav <amolyadav789@live.com>',\n  'Cc': '',\n  'Bcc': '',\n  'Date': 'Wed, 12 Jun 2024 08:03:50 +0000',\n  'Attachment_Count': 0,\n  'Mail_Body': 'From: Devendra Newaskar <devendranewaskar@gmail.com> Sent: Tuesday, June 11, 2024 2:23:19 PM To: amolyadav789@live.com <amolyadav789@live.com>; girishgadia@yahoomail.com <girishgadia@yahoomail.com> Subject: Fwd: Proposal for land > \\ufeffDear , sir This is regarding your discussion with Mr Girish gadia , We have 32 acre Rzone land at Wagholi All papers are ready, title is very clear ,plan sanctioned, 2000/- per sq ft negotiable All papers and owner is with me > Please call or mail me for further discussion > > > Thank you . > > Devendra Newaskar Gadia girlish'},\n {'Origin': 'Anant songire Classic real estate details for.eml',\n  'Subject': 'Anant songire Classic real estate details for',\n  'To': '\"nitin.valechha\" <nitin.valechha@shapoorji.com>',\n  'From': 'pushpamupadhyay.bavdhan@shapoorji.com',\n  'Cc': '',\n  'Bcc': '',\n  'Date': 'Thu, 18 Jan 2024 05:46:43 +0000',\n  'Attachment_Count': 0,\n  'Mail_Body': ''},\n {'Origin': 'Proofs of Prasad salunkhe.eml',\n  'Subject': 'Proofs of Prasad salunkhe',\n  'To': '\"ronak.maheshwari\" <ronak.maheshwari@shapoorji.com>',\n  'From': 'Pushpamupadhyay Bavdhan <pushpamupadhyay.bavdhan@shapoorji.com>',\n  'Cc': '',\n  'Bcc': '',\n  'Date': 'Thu, 07 Mar 2024 10:11:10 +0000',\n  'Attachment_Count': 0,\n  'Mail_Body': '21 January 2024- Pushpam- Prasad D Salunke- Realty network- 4 BHK- Amol - Looking for 3/4 bhk'}]"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"# Prompts","metadata":{}},{"cell_type":"code","source":"def system_prompt() -> str:\n    return \"\"\"You are an advanced AI agent designed for forensic and compliance investigations, specializing in analyzing large email datasets. Your task is to investigate multiple accusations simultaneously, searching for evidence, extracting relevant information, and drawing conclusions. You have access to a SemanticHybridSearch tool that combines Elasticsearch for keyword-based lexical searches and Faiss for semantic searches.\n\nKey Responsibilities:\n- Generate and refine search queries for multiple accusations, providing both Elasticsearch queries and semantic search strings.\n- Analyze search results to extract relevant information.\n- Evaluate evidence to determine if it supports or refutes accusations.\n- Generate conclusions based on the accumulated evidence.\n\nGuidelines:\n- Maintain objectivity and avoid bias in your analysis.\n- Consider the context and relationships between different pieces of information.\n- Be thorough in your investigation, but also efficient in your search refinement.\n- Clearly distinguish between facts, inferences, and speculations in your reports.\n- Adapt your search and analysis strategies based on the unique aspects of each accusation.\n- Utilize both lexical (Elasticsearch) and semantic (Faiss) search capabilities effectively.\n\nYou will be provided with specific instructions for each task. Always strive for accuracy, clarity, and relevance in your responses.\n\"\"\"\n\n\ndef initial_query_prompt() -> str:\n    return \"\"\"Task: Generate initial search queries for the following accusation, suitable for use with the SemanticHybridSearch tool.\n\nAccusation: {accusation_prompt}\nResponse Format: Provide the response in JSON format with the following keys:\nelastic: Contains the Elasticsearch query in JSON format.\nsemantic: Contains the semantic search query as a string.\n\nGuidelines:\nUnionized Search Approach:\n- Combine Elasticsearch and semantic search capabilities effectively. For example: Use Elasticsearch to filter specific fields (e.g., recipients, senders). Use semantic search to refine or specify the context within filtered results.\n- If only one type of search is required, leave the other key empty (e.g., {{}} for elastic or \"\" for semantic).\n\nData Schema:\n{{\n  \"Subject\": \"Subject of mail\",\n  \"To\": \"All Recipients\",\n  \"From\": \"Name of sender\",\n  \"Cc\": \"All CC\",\n  \"Bcc\": \"All BCC\",\n  \"Date\": \"Date in datetime format\",\n  \"Attachment_Count\": \"Number of attachments\",\n  \"Mail_Body\": \"Content of the mail in plain text format\"\n}}\n\nElasticsearch Query:\n- Focus on key terms and concepts relevant to the accusation.\n- Use appropriate Elasticsearch query DSL structures (e.g., bool, must, should, match, term).\n- Consider field-specific searches (e.g., subject, body, from, to) and apply boosts where necessary.\n- Ensure queries are broad enough to capture relevant information but specific enough to exclude irrelevant results.\n\nSemantic Search Query:\n- Use natural language to describe the context and meaning of the accusation.\n- Incorporate synonyms, related terms, and broader concepts to capture nuances beyond simple keywords.\n\nEfficiency and Contextual Relevance:\n- Adapt search strategies based on the unique aspects of each accusation.\n- Ensure objectivity and avoid bias in query generation.\n- Clearly distinguish between facts, inferences, and speculations.\n\nOutput Example:\n{{\n  \"elastic\": {{\n    // Elasticsearch query here\n  }},\n  \"semantic\": \"Semantic search string here\"\n}}\n\nDo not provide a preamble or an explanation, the output should strictly be in JSON format with no comments\"\"\"  # Pass\n\n\ndef refine_search_prompt() -> str:\n    return \"\"\"Task: Refine the search queries based on the current queries and extracted information to uncover more details about the accusation. Provide refined queries for both Elasticsearch and semantic search.\n\nCurrent Elasticsearch Query: {elastic_query}\nCurrent Semantic Query: {semantic_query}\nExtracted Info Summary: {info}\nAreas for Further Investigation: {areas}\nAccusation: {accusation_prompt}\n\nGuidelines:\nUnionized Search Approach:\n- Combine Elasticsearch and semantic search capabilities effectively. For example: Use Elasticsearch to filter specific fields (e.g., recipients, senders). Use semantic search to refine or specify the context within filtered results.\n- If only one type of search is required, leave the other key empty (e.g., {{}} for elastic or \"\" for semantic).\n\nData Schema:\n{{\n  \"Subject\": \"Subject of mail\",\n  \"To\": \"All Recipients\",\n  \"From\": \"Name of sender\",\n  \"Cc\": \"All CC\",\n  \"Bcc\": \"All BCC\",\n  \"Date\": \"Date in datetime format\",\n  \"Attachment_Count\": \"Number of attachments\",\n  \"Mail_Body\": \"Content of the mail in plain text format\"\n}}\n\nYour refined queries should:\n- Build upon the insights gained from the extracted information.\n- Focus on areas where evidence is lacking or inconclusive.\n- Include any new relevant terms or concepts discovered in the previous search.\n- Be more specific than the initial queries, targeting the most promising areas for further investigation.\n- Utilize Elasticsearch-specific features for the lexical query and natural language for the semantic query.\n\nRefined Search Queries:\n\n{{\n  \"elastic\": {{\n    // Elasticsearch query here\n  }},\n  \"semantic\": \"Semantic search string here\"\n}}\n\nDo not provide a preamble or an explanation, the output should strictly be in JSON format with no comments\n\"\"\"  # Pass\n\n\ndef information_extraction_prompt() -> str:\n    return \"\"\"Task: Extract relevant information from the hybrid search results related to the following accusation:\n\nAccusation: {accusation_prompt}\n\nHybrid Search Results:\n{results}\n\nAnalyze the results, which combine Elasticsearch and Faiss search outcomes. Each result contains fields like \"Subject\", \"To\", \"From\", \"Cc\", \"Bcc\", \"Date\", \"Attachment_Count\", and \"Mail_Body\".\n\nProvide the following information in JSON format:\n\n{{\n  \"accused_suspects\": [],\n  \"incident_details\": {{\n    \"events\": [\n      {{\n        \"details\": \"\",\n        \"description\": \"\",\n        \"date\": \"\",\n        \"uid\":\"\",\n      }}\n    ]\n  }},\n  \"other_parties\": {{\n    \"name\": {{\n      \"relationship\": \"\",\n      \"role\": \"\",\n      \"uid\":\"uid\",\n    }}\n  }},\n  \"summary\": \"\"\n}}\n\nEnsure all relevant information is included within this structure. Omit any explanations or additional text outside the JSON.\n\"\"\"  # Pass\n\n\ndef analyze_evidence_prompt() -> str:\n    return \"\"\"Task: Analyze the extracted information and determine if it provides sufficient evidence for the accusation. If not, suggest areas for further investigation.\n\nAccusation: {accusation_prompt}\n\nExtracted Information:\n{info}\n\nSummary of Previous Information:\n{summary}\n\nProvide your analysis in the following JSON format:\n\n{{\n  \"credibility_and_reliability\": {{\n    \"events_analysis\": [\n      {{\n        \"event\": \"Description of the event\",\n        \"credibility_score\": \"Score from 0-100\",\n        \"reasoning\": \"Explanation for the credibility score\",\n        \"uid\": \"The uid of the source where event is mentioned\"\n      }}\n    ],\n    \"relationships_analysis\": [\n      {{\n        \"entity1\": \"Name of first entity\",\n        \"entity2\": \"Name of second entity\",\n        \"relationship\": \"Description of relationship\",\n        \"credibility_impact\": \"How this relationship affects credibility\",\n        \"uid\": \"The uid of the source where entities are mentioned\"\n      }}\n    ],\n    \"overall_credibility_assessment\": \"Summary of overall credibility\"\n  }},\n  \"sufficiency\": {{\n    \"conclusion\": \"One of: sufficient, partial, insufficient\",\n    \"confidence_score\": \"Score from 0-100\",\n    \"conclusion_statement\": \"Detailed explanation of the sufficiency conclusion\",\n    \"refrences\": [\"List of the uids referenced\"]\n  }},\n  \"areas_for_further_investigation\": [\n    \"List of specific areas or questions needing further investigation\"\n  ]\n}}\n\nEnsure all relevant analysis is included within this structure. Omit any explanations or additional text outside the JSON.\n\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:20:22.111176Z","iopub.execute_input":"2024-12-19T12:20:22.111539Z","iopub.status.idle":"2024-12-19T12:20:22.117198Z","shell.execute_reply.started":"2024-12-19T12:20:22.111511Z","shell.execute_reply":"2024-12-19T12:20:22.116331Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# LLM Loading","metadata":{}},{"cell_type":"code","source":"from langchain_openai import AzureChatOpenAI\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage\nfrom langchain.prompts import PromptTemplate\n\nllm = AzureChatOpenAI(\n    azure_deployment=\"gpt-4o-mini\",  # or your deployment\n    api_version=\"2024-05-01-preview\",\n    temperature=0,\n    max_tokens=None,\n    timeout=None,\n    max_retries=2,\n    api_key=user_secrets.get_secret(\"AZURE_OPENAI_API_KEY\"),\n    azure_endpoint=user_secrets.get_secret(\"AZURE_OPENAI_ENDPOINT\"),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:33:55.584106Z","iopub.execute_input":"2024-12-19T12:33:55.584439Z","iopub.status.idle":"2024-12-19T12:33:55.938525Z","shell.execute_reply.started":"2024-12-19T12:33:55.584412Z","shell.execute_reply":"2024-12-19T12:33:55.937827Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"## LLM Sanity Check","metadata":{}},{"cell_type":"code","source":"messages = [\n    SystemMessage(\n        content=\"You are a helpful assistant! When someone says Hi you only say Bazoocar.\"\n    ),\n    HumanMessage(\n        content=\"Hi\"\n    )\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:33:58.929777Z","iopub.execute_input":"2024-12-19T12:33:58.930073Z","iopub.status.idle":"2024-12-19T12:33:58.934033Z","shell.execute_reply.started":"2024-12-19T12:33:58.930049Z","shell.execute_reply":"2024-12-19T12:33:58.933031Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"llm.invoke(messages)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:34:00.619158Z","iopub.execute_input":"2024-12-19T12:34:00.619565Z","iopub.status.idle":"2024-12-19T12:34:01.325284Z","shell.execute_reply.started":"2024-12-19T12:34:00.619530Z","shell.execute_reply":"2024-12-19T12:34:01.324587Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"AIMessage(content='Bazoocar.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 29, 'total_tokens': 33, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_5154047bf2', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-a6a6c368-a63b-4b89-aaa8-8063d4216dcb-0', usage_metadata={'input_tokens': 29, 'output_tokens': 4, 'total_tokens': 33, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"SystemMessage(\n        content= system_prompt()\n    ),","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T10:15:56.258569Z","iopub.execute_input":"2024-12-19T10:15:56.258888Z","iopub.status.idle":"2024-12-19T10:15:56.264400Z","shell.execute_reply.started":"2024-12-19T10:15:56.258865Z","shell.execute_reply":"2024-12-19T10:15:56.263549Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"(SystemMessage(content='You are an advanced AI agent designed for forensic and compliance investigations, specializing in analyzing large email datasets. Your task is to investigate multiple accusations simultaneously, searching for evidence, extracting relevant information, and drawing conclusions. You have access to a SemanticHybridSearch tool that combines Elasticsearch for keyword-based lexical searches and Faiss for semantic searches.\\n\\nKey Responsibilities:\\n- Generate and refine search queries for multiple accusations, providing both Elasticsearch queries and semantic search strings.\\n- Analyze search results to extract relevant information.\\n- Evaluate evidence to determine if it supports or refutes accusations.\\n- Generate conclusions based on the accumulated evidence.\\n\\nGuidelines:\\n- Maintain objectivity and avoid bias in your analysis.\\n- Consider the context and relationships between different pieces of information.\\n- Be thorough in your investigation, but also efficient in your search refinement.\\n- Clearly distinguish between facts, inferences, and speculations in your reports.\\n- Adapt your search and analysis strategies based on the unique aspects of each accusation.\\n- Utilize both lexical (Elasticsearch) and semantic (Faiss) search capabilities effectively.\\n\\nYou will be provided with specific instructions for each task. Always strive for accuracy, clarity, and relevance in your responses.\\n', additional_kwargs={}, response_metadata={}),)"},"metadata":{}}],"execution_count":36},{"cell_type":"markdown","source":"# Graph Code","metadata":{}},{"cell_type":"code","source":"import json\nfrom typing import Dict, List, Annotated\nfrom typing_extensions import TypedDict\nfrom langchain_core.messages import HumanMessage, AIMessage\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.output_parsers import JsonOutputParser\nfrom langgraph.graph import StateGraph, END\nfrom langgraph.prebuilt import ToolExecutor\n\nclass State(TypedDict):\n    accusation: str\n    queries:Dict[Dict, str]\n    search_results: List[Dict]\n    extracted_info: Dict\n    analysis: Dict\n    search_count: int\n\nclass InvestigationAgent:\n    def __init__(self, llm):\n        self.llm = llm\n        self.search_tool = SemanticHybridSearch(es, embedding_model, email_dict, \"/kaggle/input/llm-testing/index_tracked_elastic.json\", \"/kaggle/input/llm-testing/index_tracked_semantic.index\")\n        self.workflow = self._create_workflow()\n        self.system_prompt = SystemMessage(content=system_prompt())\n\n    def _create_workflow(self) -> StateGraph:\n        workflow = StateGraph(State)\n        workflow.add_node(\"initial_query\", self.initial_query_generation)\n        workflow.add_node(\"search\", self.perform_search)\n        workflow.add_node(\"extract_info\", self.information_extraction)\n        workflow.add_node(\"analyze\", self.evidence_analysis)\n        workflow.add_node(\"refine_query\", self.refine_query)\n\n        workflow.add_edge(\"initial_query\", \"search\")\n        workflow.add_edge(\"search\", \"extract_info\")\n        workflow.add_edge(\"extract_info\", \"analyze\")\n        workflow.add_conditional_edges(\"analyze\", self.should_continue_search, {\"end\": END, \"refine\": \"refine_query\"})\n        workflow.add_edge(\"refine_query\", \"search\")\n        workflow.set_entry_point(\"initial_query\")\n\n        return workflow.compile()\n\n    def initial_query_generation(self, state: State) -> Dict:\n        print(\"Started Execution: Initial Query Node\")\n        prompt = PromptTemplate.from_template(self._initial_query_prompt())\n        human_message = HumanMessage(content=prompt.format(accusation_prompt=state['accusation']))\n        ai_message = self.llm.invoke([self.system_prompt, human_message])\n        print(\"Tokens Used\")\n        print(ai_message.usage_metadata['input_tokens'], ai_message.usage_metadata['output_tokens'])\n        queries = json.loads(ai_message.content)\n        return {\"queries\": queries}\n\n    def perform_search(self, state: Dict) -> Dict:\n        print(\"Started Execution: Performing Search\")\n        results = self.search_tool.hybrid_search(state['queries']['elastic'], state['queries']['semantic'], (3,3))\n        return {\"search_results\": results}\n\n    def information_extraction(self, state: State) -> Dict:\n        print(\"Started Execution: Extracting Info\")\n        prompt = PromptTemplate.from_template(self._information_extraction_prompt())\n        human_message = HumanMessage(content=prompt.format(\n            accusation_prompt=state['accusation'],\n            results=json.dumps(state['search_results'])\n        ))\n        ai_message = self.llm.invoke([self.system_prompt, human_message])\n        print(\"Tokens Used\")\n        print(ai_message.usage_metadata['input_tokens'], ai_message.usage_metadata['output_tokens'])\n        extracted_info = json.loads(ai_message.content)\n        return {\"extracted_info\": extracted_info}\n\n    def evidence_analysis(self, state: State) -> Dict:\n        print(\"Started Execution: Analyzing Evidence\")\n        prompt = PromptTemplate.from_template(self._analyze_evidence_prompt())\n        human_message = HumanMessage(content=prompt.format(\n            accusation_prompt=state['accusation'],\n            info=json.dumps(state['extracted_info']),\n            summary=state[\"extracted_info\"].get('summary', 'None')\n        ))\n        ai_message = self.llm.invoke([self.system_prompt, human_message])\n        print(\"Tokens Used\")\n        print(ai_message.usage_metadata['input_tokens'], ai_message.usage_metadata['output_tokens'])\n        analysis = json.loads(ai_message.content)\n        return {\"analysis\": analysis, \"search_count\": state[\"search_count\"] + 1}\n\n    def refine_query(self, state: State) -> Dict:\n        print(\"Started Execution: Refining Search\")\n        prompt = PromptTemplate.from_template(self._refine_search_prompt())\n        human_message = HumanMessage(content=prompt.format(\n            elastic_query=json.dumps(state['queries']['elastic']),\n            semantic_query=state['queries']['semantic'],\n            info=json.dumps(state['extracted_info']),\n            areas=json.dumps(state['analysis']['areas_for_further_investigation']),\n            accusation_prompt=state['accusation']\n        ))\n        ai_message = self.llm.invoke([self.system_prompt, human_message])\n        print(\"Tokens Used\")\n        print(ai_message.usage_metadata['input_tokens'], ai_message.usage_metadata['output_tokens'])\n        refined_queries = json.loads(ai_message.content)\n        return {\"queries\": refined_queries}\n\n    def should_continue_search(self, state: State) -> str:\n        if state['search_count'] >= 2:\n            return \"end\"\n        if state['analysis']['sufficiency']['conclusion'] == \"sufficient\":\n            return \"end\"\n        # if state['search_count'] > 0 and not self._significant_difference(state['previous_analysis'], state['analysis']):\n        #     return \"end\"\n        return \"refine\"\n\n    def _significant_difference(self, prev_analysis: Dict, current_analysis: Dict) -> bool:\n        # Implement logic to compare previous and current analysis\n        # Return True if there's a significant difference, False otherwise\n        pass\n\n    def run_investigation(self, accusation: str) -> Dict:\n        inputs = {\n            \"accusation\": accusation,\n            \"search_count\": 0,\n            \"previous_analysis\": None\n        }\n        \n        for output in self.workflow.stream(inputs):\n            start = time.time()\n            if \"search_count\" in output:\n                output[\"search_count\"] += 1\n            if \"analysis\" in output:\n                output[\"previous_analysis\"] = output[\"analysis\"]\n            end = time.time()\n            print(f\"Output: {json.dumps(output, indent=2)}\")\n            print(f\"Took {end - start} seconds.\")\n            print(\"---------------------------------------------\\n\")\n        \n        return output\n\n    @staticmethod\n    def _initial_query_prompt() -> str:\n        return initial_query_prompt()\n\n    @staticmethod\n    def _refine_search_prompt() -> str:\n        return refine_search_prompt()\n\n    @staticmethod\n    def _information_extraction_prompt() -> str:\n        return information_extraction_prompt()\n\n    @staticmethod\n    def _analyze_evidence_prompt() -> str:\n        return analyze_evidence_prompt()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:37:57.364391Z","iopub.execute_input":"2024-12-19T12:37:57.364677Z","iopub.status.idle":"2024-12-19T12:37:57.381113Z","shell.execute_reply.started":"2024-12-19T12:37:57.364653Z","shell.execute_reply":"2024-12-19T12:37:57.380093Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"x = InvestigationAgent(llm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:37:58.553593Z","iopub.execute_input":"2024-12-19T12:37:58.553911Z","iopub.status.idle":"2024-12-19T12:37:58.670000Z","shell.execute_reply.started":"2024-12-19T12:37:58.553881Z","shell.execute_reply":"2024-12-19T12:37:58.669199Z"}},"outputs":[{"name":"stdout","text":"Loading Index index_tracked_semantic.index\nLoading Index index_tracked_elastic.json\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"import time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:37:55.236975Z","iopub.execute_input":"2024-12-19T12:37:55.237397Z","iopub.status.idle":"2024-12-19T12:37:55.240861Z","shell.execute_reply.started":"2024-12-19T12:37:55.237353Z","shell.execute_reply":"2024-12-19T12:37:55.240096Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"x.run_investigation(\"Employee is misbehaving, has been rude and indisciplined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:38:51.841872Z","iopub.execute_input":"2024-12-19T12:38:51.842167Z","iopub.status.idle":"2024-12-19T12:39:07.093305Z","shell.execute_reply.started":"2024-12-19T12:38:51.842143Z","shell.execute_reply":"2024-12-19T12:39:07.092621Z"}},"outputs":[{"name":"stdout","text":"Started Execution: Initial Query Node\nTokens Used\n687 253\nOutput: {\n  \"initial_query\": {\n    \"queries\": {\n      \"elastic\": {\n        \"query\": {\n          \"bool\": {\n            \"must\": [\n              {\n                \"bool\": {\n                  \"should\": [\n                    {\n                      \"match\": {\n                        \"Mail_Body\": {\n                          \"query\": \"rude\",\n                          \"boost\": 2\n                        }\n                      }\n                    },\n                    {\n                      \"match\": {\n                        \"Mail_Body\": {\n                          \"query\": \"misbehaving\",\n                          \"boost\": 2\n                        }\n                      }\n                    },\n                    {\n                      \"match\": {\n                        \"Mail_Body\": {\n                          \"query\": \"indisciplined\",\n                          \"boost\": 2\n                        }\n                      }\n                    },\n                    {\n                      \"match\": {\n                        \"Mail_Body\": {\n                          \"query\": \"unprofessional\",\n                          \"boost\": 1.5\n                        }\n                      }\n                    },\n                    {\n                      \"match\": {\n                        \"Mail_Body\": {\n                          \"query\": \"disrespectful\",\n                          \"boost\": 1.5\n                        }\n                      }\n                    }\n                  ]\n                }\n              }\n            ],\n            \"filter\": [\n              {\n                \"exists\": {\n                  \"field\": \"From\"\n                }\n              },\n              {\n                \"exists\": {\n                  \"field\": \"To\"\n                }\n              }\n            ]\n          }\n        }\n      },\n      \"semantic\": \"Search for instances of employee misconduct, including rudeness, indiscipline, and unprofessional behavior in emails.\"\n    }\n  }\n}\nTook 1.1920928955078125e-06 seconds.\n---------------------------------------------\n\nStarted Execution: Performing Search\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"becb457cdb364e29b225b62184a5e4c3"}},"metadata":{}},{"name":"stdout","text":"Output: {\n  \"search\": {\n    \"search_results\": [\n      {\n        \"Origin\": \"Unprofessional Behavior & extension of leaves.eml\",\n        \"Subject\": \"Unprofessional Behavior & extension of leaves\",\n        \"To\": \"Pushpamupadhyay Bavdhan <pushpamupadhyay.bavdhan@shapoorji.com>\",\n        \"From\": \"\\\"ronak.maheshwari\\\" <ronak.maheshwari@shapoorji.com>\",\n        \"Cc\": \"\\\"shoubhik.bhattacharya\\\" <shoubhik.bhattacharya@shapoorji.com>, \\\"pritesh.jain\\\" <pritesh.jain@shapoorji.com>\",\n        \"Bcc\": \"\",\n        \"Date\": \"Sun, 31 Mar 2024 08:03:25 +0000\",\n        \"Attachment_Count\": 0,\n        \"Mail_Body\": \"Hi Pushpam, Hope you\\u2019re doing well ! As you were on planned leave from 25th to 31st of March 2024 for 7 days which was pre-approved by us. You leaves was well planned by you & approved in advance verbally by Shoubhik back in February & in March by me & Pritesh Sir. During these days you have been unapproachable for some important office related update. Even to CPs also, this is very much unprofessional approach. I understand your phone isn\\u2019t working but still you haven\\u2019t got replacement of phone and no curtesy to call back yet. Also CPs have complained you have been unresponsive during these days. Also, you gave me the reason of your mother not well, I pray the speedy recovery for her. However, even during such a difficult time you have been roaming around the cities and seen partying on your social media, which is little strange during such difficult times, Lastly you knew you had to report to office by 1st of April 2024 but today you\\u2019re dropping an msg that you will resume to office on wed i.e.3rd of April by stating your \\u2018not getting return tickets ? which additional 2 days more leaves total 9 days. Despite the fact that you was given leaves well in advance. You will require to report office by tomorrow by taking bus/train/flight. Failure to do so you will be given show cause reason. Regards,RM\"\n      },\n      {\n        \"Origin\": \"Unprofessional Behavior & extension of leaves.eml\",\n        \"Subject\": \"Unprofessional Behavior & extension of leaves\",\n        \"To\": \"Pushpamupadhyay Bavdhan <pushpamupadhyay.bavdhan@shapoorji.com>\",\n        \"From\": \"\\\"ronak.maheshwari\\\" <ronak.maheshwari@shapoorji.com>\",\n        \"Cc\": \"\\\"shoubhik.bhattacharya\\\" <shoubhik.bhattacharya@shapoorji.com>, \\\"pritesh.jain\\\" <pritesh.jain@shapoorji.com>\",\n        \"Bcc\": \"\",\n        \"Date\": \"Sun, 31 Mar 2024 08:03:25 +0000\",\n        \"Attachment_Count\": 0,\n        \"Mail_Body\": \"\"\n      },\n      {\n        \"Origin\": \"Unprofessional Behavior - Pushapm U.eml\",\n        \"Subject\": \"Unprofessional Behavior - Pushapm U\",\n        \"To\": \"\\\"shoubhik.bhattacharya\\\" <shoubhik.bhattacharya@shapoorji.com>\",\n        \"From\": \"\\\"ronak.maheshwari\\\" <ronak.maheshwari@shapoorji.com>\",\n        \"Cc\": \"\\\"pritesh.jain\\\" <pritesh.jain@shapoorji.com>, Pushpamupadhyay Bavdhan <pushpamupadhyay.bavdhan@shapoorji.com>\",\n        \"Bcc\": \"\",\n        \"Date\": \"Fri, 14 Jun 2024 08:23:28 +0000\",\n        \"Attachment_Count\": 0,\n        \"Mail_Body\": \"Hi Shoubhik & Sir, This is to inform you that Pushpam has shown vigorous behavior in last couple of days for one CP Prophunter. He has been going aggressive on face in office & not able to understand the right way approach. Despite multiple understanding & warning he has been self-pressuring on Pranjay & with me. Please give him final warning or I may have to raise his unprofessional behavior with colleagues to HR. Synopsis:Pushpam has a CP Prophunter in his bucket, and I have told him many times to be in touch with RMs along with CP owners & shown me calling remarks on printout since this month he is active on walk ins with only 1 CP which is alarming situation but instead of reaching out to CPs & showing us remarks, he chooses to be num. On last Sunday, CP Prophunter relationship manager has induced the site visit with coordination through Pranjay & he has posted in our internal sourcing group that Prophunter client at site at 5:15pm.And failed to raise the objection, later Tuesday he has realized that this is his CP, which I said I won\\u2019t give to you now because you are not aware that your CPs are inducing Site visits. He has argued with me & I have given him explanation, but he has failed to understand & creative noise everyday with me. This is not the the first time, in past also he wasn\\u2019t aware of his CP kitty inducing walk ins. If any not active CP is not active on walk ins & they are just holding on their name by stating its my CP. Then this won\\u2019t help in business. He has to cool down his tone while discussing the issue & not to drag co partners in vague arguments\"\n      },\n      {\n        \"Origin\": \"Behavioral and Disciplinary Concerns- Pushpam Updhyay.eml\",\n        \"Subject\": \"Behavioral and Disciplinary Concerns- Pushpam Updhyay\",\n        \"To\": \"Pushpamupadhyay Bavdhan <pushpamupadhyay.bavdhan@shapoorji.com>\",\n        \"From\": \"\\\"shoubhik.bhattacharya\\\" <shoubhik.bhattacharya@shapoorji.com>\",\n        \"Cc\": \"\\\"pritesh.jain\\\" <pritesh.jain@shapoorji.com>, \\\"ronak.maheshwari\\\" <ronak.maheshwari@shapoorji.com>\",\n        \"Bcc\": \"\",\n        \"Date\": \"Sun, 07 Jul 2024 06:17:10 +0000\",\n        \"Attachment_Count\": 0,\n        \"Mail_Body\": \"Dear Pushpam, As discussed, we have observed several behavioural and disciplinary issues. Despite multiple reminders, you have not been following team directions. Specific instances include shouting at your senior, not reaching the office on time (06-07-2024), and absconding from work (05-07-2024). There are other previous issues as well. All your seniors have tried to make you understand, but you have not been following the direction. If these issues are not addressed or controlled from your end, we will have no choice but to escalate this matter to HR. Thank you for your attention to this matter. Best regards, Shoubhik Bhattacharya\"\n      },\n      {\n        \"Origin\": \"Unprofessional Behavior - Pushapm U.eml\",\n        \"Subject\": \"Unprofessional Behavior - Pushapm U \",\n        \"To\": \"\\\"shoubhik.bhattacharya\\\" <shoubhik.bhattacharya@shapoorji.com>\",\n        \"From\": \"\\\"ronak.maheshwari\\\" <ronak.maheshwari@shapoorji.com>\",\n        \"Cc\": \"\\\"pritesh.jain\\\" <pritesh.jain@shapoorji.com>, Pushpamupadhyay Bavdhan <pushpamupadhyay.bavdhan@shapoorji.com>\",\n        \"Bcc\": \"\",\n        \"Date\": \"Fri, 14 Jun 2024 08:23:28 +0000\",\n        \"Attachment_Count\": 0,\n        \"Mail_Body\": \"Hi Shoubhik & Sir, This is to inform you that Pushpam has shown vigorous behavior in last couple of days for one CP Prophunter. He has been going aggressive on face in office & not able to understand the right way approach. Despite multiple understanding & warning he has been self-pressuring on Pranjay & with me. Please give him final warning or I may have to raise his unprofessional behavior with colleagues to HR. Synopsis:Pushpam has a CP Prophunter in his bucket, and I have told him many times to be in touch with RMs along with CP owners & shown me calling remarks on printout since this month he is active on walk ins with only 1 CP which is alarming situation but instead of reaching out to CPs & showing us remarks, he chooses to be num. On last Sunday, CP Prophunter relationship manager has induced the site visit with coordination through Pranjay & he has posted in our internal sourcing group that Prophunter client at site at 5:15pm.And failed to raise the objection, later Tuesday he has realized that this is his CP, which I said I won\\u2019t give to you now because you are not aware that your CPs are inducing Site visits. He has argued with me & I have given him explanation, but he has failed to understand & creative noise everyday with me. This is not the the first time, in past also he wasn\\u2019t aware of his CP kitty inducing walk ins. If any not active CP is not active on walk ins & they are just holding on their name by stating its my CP. Then this won\\u2019t help in business. He has to cool down his tone while discussing the issue & not to drag co partners in vague arguments\"\n      },\n      {\n        \"Origin\": \"Unprofessional Behavior & extension of leaves.eml\",\n        \"Subject\": \"Unprofessional Behavior & extension of leaves  \",\n        \"To\": \"Pushpamupadhyay Bavdhan <pushpamupadhyay.bavdhan@shapoorji.com>\",\n        \"From\": \"\\\"ronak.maheshwari\\\" <ronak.maheshwari@shapoorji.com>\",\n        \"Cc\": \"\\\"shoubhik.bhattacharya\\\" <shoubhik.bhattacharya@shapoorji.com>, \\\"pritesh.jain\\\" <pritesh.jain@shapoorji.com>\",\n        \"Bcc\": \"\",\n        \"Date\": \"Sun, 31 Mar 2024 08:03:25 +0000\",\n        \"Attachment_Count\": 0,\n        \"Mail_Body\": \"\"\n      }\n    ]\n  }\n}\nTook 9.5367431640625e-07 seconds.\n---------------------------------------------\n\nStarted Execution: Extracting Info\nTokens Used\n2630 488\nOutput: {\n  \"extract_info\": {\n    \"extracted_info\": {\n      \"accused_suspects\": [\n        {\n          \"name\": \"Pushpamupadhyay Bavdhan\",\n          \"uid\": \"pushpamupadhyay.bavdhan@shapoorji.com\"\n        }\n      ],\n      \"incident_details\": {\n        \"events\": [\n          {\n            \"details\": \"Pushpam has shown unprofessional behavior, including being unapproachable during approved leave and failing to communicate with colleagues.\",\n            \"description\": \"Complaints about unresponsiveness and aggressive behavior towards colleagues.\",\n            \"date\": \"31 Mar 2024\",\n            \"uid\": \"Unprofessional Behavior & extension of leaves.eml\"\n          },\n          {\n            \"details\": \"Pushpam has been reported for shouting at a senior and not following team directions.\",\n            \"description\": \"Multiple reminders ignored regarding behavioral and disciplinary issues.\",\n            \"date\": \"07 Jul 2024\",\n            \"uid\": \"Behavioral and Disciplinary Concerns- Pushpam Updhyay.eml\"\n          },\n          {\n            \"details\": \"Pushpam has been aggressive towards colleagues and has failed to understand work-related instructions.\",\n            \"description\": \"Final warning issued due to ongoing unprofessional behavior.\",\n            \"date\": \"14 Jun 2024\",\n            \"uid\": \"Unprofessional Behavior - Pushapm U.eml\"\n          }\n        ]\n      },\n      \"other_parties\": {\n        \"ronak.maheshwari\": {\n          \"relationship\": \"Colleague\",\n          \"role\": \"Sender of complaints\",\n          \"uid\": \"ronak.maheshwari@shapoorji.com\"\n        },\n        \"shoubhik.bhattacharya\": {\n          \"relationship\": \"Colleague\",\n          \"role\": \"Cc recipient and involved in discussions\",\n          \"uid\": \"shoubhik.bhattacharya@shapoorji.com\"\n        },\n        \"pritesh.jain\": {\n          \"relationship\": \"Colleague\",\n          \"role\": \"Cc recipient\",\n          \"uid\": \"pritesh.jain@shapoorji.com\"\n        }\n      },\n      \"summary\": \"Pushpamupadhyay Bavdhan has been accused of misbehaving, being rude, and showing indiscipline through unprofessional behavior, including being unresponsive during approved leave, aggressive interactions with colleagues, and failure to follow team directions despite multiple warnings.\"\n    }\n  }\n}\nTook 1.430511474609375e-06 seconds.\n---------------------------------------------\n\nStarted Execution: Analyzing Evidence\nTokens Used\n1056 768\nOutput: {\n  \"analyze\": {\n    \"analysis\": {\n      \"credibility_and_reliability\": {\n        \"events_analysis\": [\n          {\n            \"event\": \"Complaints about unresponsiveness and aggressive behavior towards colleagues.\",\n            \"credibility_score\": 75,\n            \"reasoning\": \"The event is supported by multiple complaints from colleagues, indicating a pattern of behavior.\",\n            \"uid\": \"Unprofessional Behavior & extension of leaves.eml\"\n          },\n          {\n            \"event\": \"Pushpam has been reported for shouting at a senior and not following team directions.\",\n            \"credibility_score\": 80,\n            \"reasoning\": \"This incident involves a senior colleague, which adds weight to the seriousness of the behavior.\",\n            \"uid\": \"Behavioral and Disciplinary Concerns- Pushpam Updhyay.eml\"\n          },\n          {\n            \"event\": \"Final warning issued due to ongoing unprofessional behavior.\",\n            \"credibility_score\": 85,\n            \"reasoning\": \"The issuance of a final warning suggests that the behavior was serious enough to warrant formal action.\",\n            \"uid\": \"Unprofessional Behavior - Pushapm U.eml\"\n          }\n        ],\n        \"relationships_analysis\": [\n          {\n            \"entity1\": \"Pushpamupadhyay Bavdhan\",\n            \"entity2\": \"Ronak Maheshwari\",\n            \"relationship\": \"Colleague who sent complaints about Pushpam's behavior.\",\n            \"credibility_impact\": \"Ronak's complaints provide direct evidence of Pushpam's alleged misbehavior.\",\n            \"uid\": \"ronak.maheshwari@shapoorji.com\"\n          },\n          {\n            \"entity1\": \"Pushpamupadhyay Bavdhan\",\n            \"entity2\": \"Shoubhik Bhattacharya\",\n            \"relationship\": \"Cc recipient involved in discussions regarding Pushpam's behavior.\",\n            \"credibility_impact\": \"Shoubhik's involvement may indicate corroboration of the complaints.\",\n            \"uid\": \"shoubhik.bhattacharya@shapoorji.com\"\n          },\n          {\n            \"entity1\": \"Pushpamupadhyay Bavdhan\",\n            \"entity2\": \"Pritesh Jain\",\n            \"relationship\": \"Cc recipient who may have additional insights into the situation.\",\n            \"credibility_impact\": \"Pritesh's perspective could provide further context to the accusations.\",\n            \"uid\": \"pritesh.jain@shapoorji.com\"\n          }\n        ],\n        \"overall_credibility_assessment\": \"The credibility of the accusations is supported by multiple incidents and corroborating relationships, indicating a consistent pattern of unprofessional behavior.\"\n      },\n      \"sufficiency\": {\n        \"conclusion\": \"sufficient\",\n        \"confidence_score\": 90,\n        \"conclusion_statement\": \"The evidence collected from multiple incidents and corroborating testimonies from colleagues provides a strong basis for the accusations of misbehavior, rudeness, and indiscipline.\",\n        \"refrences\": [\n          \"Unprofessional Behavior & extension of leaves.eml\",\n          \"Behavioral and Disciplinary Concerns- Pushpam Updhyay.eml\",\n          \"Unprofessional Behavior - Pushapm U.eml\",\n          \"ronak.maheshwari@shapoorji.com\",\n          \"shoubhik.bhattacharya@shapoorji.com\",\n          \"pritesh.jain@shapoorji.com\"\n        ]\n      },\n      \"areas_for_further_investigation\": [\n        \"Gather additional testimonies from other colleagues who may have witnessed Pushpam's behavior.\",\n        \"Review any previous performance evaluations or disciplinary records for Pushpam.\",\n        \"Investigate the context of the incidents to determine if external factors may have influenced Pushpam's behavior.\"\n      ]\n    },\n    \"search_count\": 1\n  }\n}\nTook 1.1920928955078125e-06 seconds.\n---------------------------------------------\n\n","output_type":"stream"},{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"{'analyze': {'analysis': {'credibility_and_reliability': {'events_analysis': [{'event': 'Complaints about unresponsiveness and aggressive behavior towards colleagues.',\n      'credibility_score': 75,\n      'reasoning': 'The event is supported by multiple complaints from colleagues, indicating a pattern of behavior.',\n      'uid': 'Unprofessional Behavior & extension of leaves.eml'},\n     {'event': 'Pushpam has been reported for shouting at a senior and not following team directions.',\n      'credibility_score': 80,\n      'reasoning': 'This incident involves a senior colleague, which adds weight to the seriousness of the behavior.',\n      'uid': 'Behavioral and Disciplinary Concerns- Pushpam Updhyay.eml'},\n     {'event': 'Final warning issued due to ongoing unprofessional behavior.',\n      'credibility_score': 85,\n      'reasoning': 'The issuance of a final warning suggests that the behavior was serious enough to warrant formal action.',\n      'uid': 'Unprofessional Behavior - Pushapm U.eml'}],\n    'relationships_analysis': [{'entity1': 'Pushpamupadhyay Bavdhan',\n      'entity2': 'Ronak Maheshwari',\n      'relationship': \"Colleague who sent complaints about Pushpam's behavior.\",\n      'credibility_impact': \"Ronak's complaints provide direct evidence of Pushpam's alleged misbehavior.\",\n      'uid': 'ronak.maheshwari@shapoorji.com'},\n     {'entity1': 'Pushpamupadhyay Bavdhan',\n      'entity2': 'Shoubhik Bhattacharya',\n      'relationship': \"Cc recipient involved in discussions regarding Pushpam's behavior.\",\n      'credibility_impact': \"Shoubhik's involvement may indicate corroboration of the complaints.\",\n      'uid': 'shoubhik.bhattacharya@shapoorji.com'},\n     {'entity1': 'Pushpamupadhyay Bavdhan',\n      'entity2': 'Pritesh Jain',\n      'relationship': 'Cc recipient who may have additional insights into the situation.',\n      'credibility_impact': \"Pritesh's perspective could provide further context to the accusations.\",\n      'uid': 'pritesh.jain@shapoorji.com'}],\n    'overall_credibility_assessment': 'The credibility of the accusations is supported by multiple incidents and corroborating relationships, indicating a consistent pattern of unprofessional behavior.'},\n   'sufficiency': {'conclusion': 'sufficient',\n    'confidence_score': 90,\n    'conclusion_statement': 'The evidence collected from multiple incidents and corroborating testimonies from colleagues provides a strong basis for the accusations of misbehavior, rudeness, and indiscipline.',\n    'refrences': ['Unprofessional Behavior & extension of leaves.eml',\n     'Behavioral and Disciplinary Concerns- Pushpam Updhyay.eml',\n     'Unprofessional Behavior - Pushapm U.eml',\n     'ronak.maheshwari@shapoorji.com',\n     'shoubhik.bhattacharya@shapoorji.com',\n     'pritesh.jain@shapoorji.com']},\n   'areas_for_further_investigation': [\"Gather additional testimonies from other colleagues who may have witnessed Pushpam's behavior.\",\n    'Review any previous performance evaluations or disciplinary records for Pushpam.',\n    \"Investigate the context of the incidents to determine if external factors may have influenced Pushpam's behavior.\"]},\n  'search_count': 1}}"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"x.system_prompt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T12:23:30.455069Z","iopub.execute_input":"2024-12-19T12:23:30.455378Z","iopub.status.idle":"2024-12-19T12:23:30.460641Z","shell.execute_reply.started":"2024-12-19T12:23:30.455352Z","shell.execute_reply":"2024-12-19T12:23:30.459858Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"SystemMessage(content='You are an advanced AI agent designed for forensic and compliance investigations, specializing in analyzing large email datasets. Your task is to investigate multiple accusations simultaneously, searching for evidence, extracting relevant information, and drawing conclusions. You have access to a SemanticHybridSearch tool that combines Elasticsearch for keyword-based lexical searches and Faiss for semantic searches.\\n\\nKey Responsibilities:\\n- Generate and refine search queries for multiple accusations, providing both Elasticsearch queries and semantic search strings.\\n- Analyze search results to extract relevant information.\\n- Evaluate evidence to determine if it supports or refutes accusations.\\n- Generate conclusions based on the accumulated evidence.\\n\\nGuidelines:\\n- Maintain objectivity and avoid bias in your analysis.\\n- Consider the context and relationships between different pieces of information.\\n- Be thorough in your investigation, but also efficient in your search refinement.\\n- Clearly distinguish between facts, inferences, and speculations in your reports.\\n- Adapt your search and analysis strategies based on the unique aspects of each accusation.\\n- Utilize both lexical (Elasticsearch) and semantic (Faiss) search capabilities effectively.\\n\\nYou will be provided with specific instructions for each task. Always strive for accuracy, clarity, and relevance in your responses.\\n', additional_kwargs={}, response_metadata={})"},"metadata":{}}],"execution_count":18}]}