{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-11T11:16:54.148312Z",
     "iopub.status.busy": "2024-12-11T11:16:54.148077Z",
     "iopub.status.idle": "2024-12-11T11:17:14.499036Z",
     "shell.execute_reply": "2024-12-11T11:17:14.497958Z",
     "shell.execute_reply.started": "2024-12-11T11:16:54.148275Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install elasticsearch\n",
    "!pip install faiss-cpu sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-input": false
   },
   "source": [
    "## Initialize Elastic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T11:17:18.352216Z",
     "iopub.status.busy": "2024-12-11T11:17:18.351850Z",
     "iopub.status.idle": "2024-12-11T11:17:18.356701Z",
     "shell.execute_reply": "2024-12-11T11:17:18.355867Z",
     "shell.execute_reply.started": "2024-12-11T11:17:18.352182Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T11:17:20.781650Z",
     "iopub.status.busy": "2024-12-11T11:17:20.780794Z",
     "iopub.status.idle": "2024-12-11T11:17:52.586234Z",
     "shell.execute_reply": "2024-12-11T11:17:52.585081Z",
     "shell.execute_reply.started": "2024-12-11T11:17:20.781606Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-8.16.1-linux-x86_64.tar.gz -q\n",
    "!tar -xzf elasticsearch-8.16.1-linux-x86_64.tar.gz\n",
    "!chown -R daemon:daemon elasticsearch-8.16.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T11:17:57.350209Z",
     "iopub.status.busy": "2024-12-11T11:17:57.349507Z",
     "iopub.status.idle": "2024-12-11T11:17:57.363698Z",
     "shell.execute_reply": "2024-12-11T11:17:57.362452Z",
     "shell.execute_reply.started": "2024-12-11T11:17:57.350171Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Creating daemon instance of elasticsearch\n",
    "import os\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "es_server = Popen(['elasticsearch-8.16.1/bin/elasticsearch'], \n",
    "                  stdout=PIPE, stderr=STDOUT,\n",
    "                  preexec_fn=lambda: os.setuid(1)  # as daemon\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T11:18:07.334542Z",
     "iopub.status.busy": "2024-12-11T11:18:07.333929Z",
     "iopub.status.idle": "2024-12-11T11:18:09.211492Z",
     "shell.execute_reply": "2024-12-11T11:18:09.205032Z",
     "shell.execute_reply.started": "2024-12-11T11:18:07.334485Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ps -ef | grep elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2024-12-11T11:18:15.296016Z",
     "iopub.status.busy": "2024-12-11T11:18:15.295200Z",
     "iopub.status.idle": "2024-12-11T11:18:16.538192Z",
     "shell.execute_reply": "2024-12-11T11:18:16.536293Z",
     "shell.execute_reply.started": "2024-12-11T11:18:15.295955Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "new_yml = \"\"\"# ======================== Elasticsearch Configuration =========================\n",
    "#\n",
    "# NOTE: Elasticsearch comes with reasonable defaults for most settings.\n",
    "#       Before you set out to tweak and tune the configuration, make sure you\n",
    "#       understand what are you trying to accomplish and the consequences.\n",
    "#\n",
    "# The primary way of configuring a node is via this file. This template lists\n",
    "# the most important settings you may want to configure for a production cluster.\n",
    "#\n",
    "# Please consult the documentation for further information on configuration options:\n",
    "# https://www.elastic.co/guide/en/elasticsearch/reference/index.html\n",
    "#\n",
    "# ---------------------------------- Cluster -----------------------------------\n",
    "#\n",
    "# Use a descriptive name for your cluster:\n",
    "#\n",
    "#cluster.name: my-application\n",
    "#\n",
    "# ------------------------------------ Node ------------------------------------\n",
    "#\n",
    "# Use a descriptive name for the node:\n",
    "#\n",
    "#node.name: node-1\n",
    "#\n",
    "# Add custom attributes to the node:\n",
    "#\n",
    "#node.attr.rack: r1\n",
    "#\n",
    "# ----------------------------------- Paths ------------------------------------\n",
    "#\n",
    "# Path to directory where to store the data (separate multiple locations by comma):\n",
    "#\n",
    "#path.data: /path/to/data\n",
    "#\n",
    "# Path to log files:\n",
    "#\n",
    "#path.logs: /path/to/logs\n",
    "#\n",
    "# ----------------------------------- Memory -----------------------------------\n",
    "#\n",
    "# Lock the memory on startup:\n",
    "#\n",
    "#bootstrap.memory_lock: true\n",
    "#\n",
    "# Make sure that the heap size is set to about half the memory available\n",
    "# on the system and that the owner of the process is allowed to use this\n",
    "# limit.\n",
    "#\n",
    "# Elasticsearch performs poorly when the system is swapping the memory.\n",
    "#\n",
    "# ---------------------------------- Network -----------------------------------\n",
    "#\n",
    "# By default Elasticsearch is only accessible on localhost. Set a different\n",
    "# address here to expose this node on the network:\n",
    "#\n",
    "#network.host: 192.168.0.1\n",
    "#\n",
    "# By default Elasticsearch listens for HTTP traffic on the first free port it\n",
    "# finds starting at 9200. Set a specific HTTP port here:\n",
    "#\n",
    "#http.port: 9200\n",
    "#\n",
    "# For more information, consult the network module documentation.\n",
    "#\n",
    "# --------------------------------- Discovery ----------------------------------\n",
    "#\n",
    "# Pass an initial list of hosts to perform discovery when this node is started:\n",
    "# The default list of hosts is [\"127.0.0.1\", \"[::1]\"]\n",
    "#\n",
    "#discovery.seed_hosts: [\"host1\", \"host2\"]\n",
    "#\n",
    "# Bootstrap the cluster using an initial set of master-eligible nodes:\n",
    "#\n",
    "#cluster.initial_master_nodes: [\"node-1\", \"node-2\"]\n",
    "#\n",
    "# For more information, consult the discovery and cluster formation module documentation.\n",
    "#\n",
    "# ---------------------------------- Various -----------------------------------\n",
    "#\n",
    "# Allow wildcard deletion of indices:\n",
    "#\n",
    "#action.destructive_requires_name: false\n",
    "\n",
    "#----------------------- BEGIN SECURITY AUTO CONFIGURATION -----------------------\n",
    "#\n",
    "# The following settings, TLS certificates, and keys have been automatically      \n",
    "# generated to configure Elasticsearch security features on 11-12-2024 05:30:04\n",
    "#\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# Enable security features\n",
    "xpack.security.enabled: false\n",
    "\n",
    "xpack.security.enrollment.enabled: false\n",
    "\n",
    "# Enable encryption for HTTP API client connections, such as Kibana, Logstash, and Agents\n",
    "xpack.security.http.ssl:\n",
    "  enabled: false\n",
    "  keystore.path: certs/http.p12\n",
    "\n",
    "# Enable encryption and mutual authentication between cluster nodes\n",
    "xpack.security.transport.ssl:\n",
    "  enabled: false\n",
    "  verification_mode: certificate\n",
    "  keystore.path: certs/transport.p12\n",
    "  truststore.path: certs/transport.p12\n",
    "# Create a new cluster with the current node only\n",
    "# Additional nodes can still join the cluster later\n",
    "cluster.initial_master_nodes: [\"63410036aadd\"]\n",
    "\n",
    "# Allow HTTP API connections from anywhere\n",
    "# Connections are encrypted and require user authentication\n",
    "http.host: 0.0.0.0\n",
    "\n",
    "# Allow other nodes to join the cluster from anywhere\n",
    "# Connections are encrypted and mutually authenticated\n",
    "#transport.host: 0.0.0.0\n",
    "\n",
    "#----------------------- END SECURITY AUTO CONFIGURATION -------------------------\"\"\"\n",
    "\n",
    "old_yml = open(\"/kaggle/working/elasticsearch-8.16.1/config/elasticsearch.yml\", \"w\")\n",
    "old_yml.write(new_yml)\n",
    "old_yml.close()\n",
    "!tail -n 25 \"/kaggle/working/elasticsearch-8.16.1/config/elasticsearch.yml\" | tac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T11:18:20.780071Z",
     "iopub.status.busy": "2024-12-11T11:18:20.779552Z",
     "iopub.status.idle": "2024-12-11T11:18:20.790879Z",
     "shell.execute_reply": "2024-12-11T11:18:20.788462Z",
     "shell.execute_reply.started": "2024-12-11T11:18:20.780021Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "es_server.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T11:20:28.638737Z",
     "iopub.status.busy": "2024-12-11T11:20:28.638028Z",
     "iopub.status.idle": "2024-12-11T11:20:29.621707Z",
     "shell.execute_reply": "2024-12-11T11:20:29.620753Z",
     "shell.execute_reply.started": "2024-12-11T11:20:28.638701Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!kill -15 301"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T11:20:56.402338Z",
     "iopub.status.busy": "2024-12-11T11:20:56.401851Z",
     "iopub.status.idle": "2024-12-11T11:20:57.596311Z",
     "shell.execute_reply": "2024-12-11T11:20:57.595021Z",
     "shell.execute_reply.started": "2024-12-11T11:20:56.402286Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ps -ef | grep elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T11:20:34.986328Z",
     "iopub.status.busy": "2024-12-11T11:20:34.985417Z",
     "iopub.status.idle": "2024-12-11T11:20:35.008232Z",
     "shell.execute_reply": "2024-12-11T11:20:35.006844Z",
     "shell.execute_reply.started": "2024-12-11T11:20:34.986290Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "es_server = Popen(['elasticsearch-8.16.1/bin/elasticsearch'], \n",
    "                  stdout=PIPE, stderr=STDOUT,\n",
    "                  preexec_fn=lambda: os.setuid(1)  # as daemon\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T11:21:00.045818Z",
     "iopub.status.busy": "2024-12-11T11:21:00.045044Z",
     "iopub.status.idle": "2024-12-11T11:21:00.145531Z",
     "shell.execute_reply": "2024-12-11T11:21:00.144680Z",
     "shell.execute_reply.started": "2024-12-11T11:21:00.045785Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Elasticsearch client\n",
    "es = Elasticsearch(\"http://localhost:9200/\")\n",
    "\n",
    "if es.ping():\n",
    "    print(\"Connected to Elasticsearch!\")\n",
    "    try:\n",
    "        response = es.indices.delete(index=\"emails\")\n",
    "        print(f\"Successfully deleted index: emails\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting index/Index not found: {e}\")\n",
    "else:\n",
    "    print(\"Connection failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:02:18.523068Z",
     "iopub.status.busy": "2024-12-11T12:02:18.522720Z",
     "iopub.status.idle": "2024-12-11T12:02:18.540419Z",
     "shell.execute_reply": "2024-12-11T12:02:18.539619Z",
     "shell.execute_reply.started": "2024-12-11T12:02:18.523040Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "import json\n",
    "from elasticsearch.helpers import bulk\n",
    "from difflib import SequenceMatcher\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "import re\n",
    "\n",
    "class SemanticHybridSearch:\n",
    "    \"\"\"\n",
    "    A class that combines Elasticsearch for keyword-based lexical searches and Faiss for semantic searches.\n",
    "\n",
    "    This class provides methods to load and search both Elasticsearch and Faiss indices,\n",
    "    as well as a hybrid search method that combines results from both search types.\n",
    "\n",
    "    Attributes:\n",
    "        data (list): The dataset used for searching.\n",
    "        es_client (Elasticsearch): Elasticsearch client for performing lexical searches.\n",
    "        embedding_model: Model used for encoding queries into embeddings.\n",
    "        vector_index (faiss.Index): Faiss index for semantic searches.\n",
    "        elastic_index_name (str): Name of the Elasticsearch index.\n",
    "    \"\"\"\n",
    "    def __init__(self, es_client, embedding_model, data: list, elastic_index_path: str, vector_index_path: str):\n",
    "        \"\"\"\n",
    "        Initialize the SemanticHybridSearch class.\n",
    "\n",
    "        Args:\n",
    "            es_client (Elasticsearch): Elasticsearch client.\n",
    "            embedding_model: Model for encoding queries into embeddings.\n",
    "            data (list): Dataset used for searching.\n",
    "            elastic_index_path (str): Path to the Elasticsearch index file.\n",
    "            vector_index_path (str): Path to the Faiss vector index file.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.es_client = es_client\n",
    "        self.embedding_model = embedding_model\n",
    "        self.vector_index = self.load_vector_index(vector_index_path)\n",
    "        self.elastic_index = self.load_elastic_index(elastic_index_path)\n",
    "\n",
    "        self.elastic_index_name = \"\"\n",
    "    \n",
    "    def load_elastic_index(self, elastic_index_path: str):\n",
    "        \"\"\"\n",
    "        Load the Elasticsearch index from a file.\n",
    "\n",
    "        Args:\n",
    "            elastic_index_path (str): Path to the Elasticsearch index file.\n",
    "        \"\"\"\n",
    "        with open(elastic_index_path) as f:\n",
    "            documents = json.load(f)\n",
    "            self.elastic_index_name = os.path.basename(elastic_index_path)\n",
    "            print(f\"Loading Index {self.elastic_index_name}\")\n",
    "\n",
    "            actions = [\n",
    "                {\n",
    "                    \"_index\": self.elastic_index_name,\n",
    "                    \"_id\": doc[\"_id\"],\n",
    "                    \"_source\": doc[\"_source\"]\n",
    "                }\n",
    "                for doc in documents\n",
    "            ]\n",
    "            bulk(self.es_client, actions)\n",
    "\n",
    "    def load_vector_index(self, vector_index_path: str):\n",
    "        \"\"\"\n",
    "        Load the Faiss vector index from a file.\n",
    "\n",
    "        Args:\n",
    "            vector_index_path (str): Path to the Faiss vector index file.\n",
    "\n",
    "        Returns:\n",
    "            faiss.Index: Loaded Faiss index.\n",
    "        \"\"\"\n",
    "        print(f\"Loading Index {os.path.basename(vector_index_path)}\")\n",
    "        index = faiss.read_index(vector_index_path)\n",
    "        return index\n",
    "\n",
    "    def elastic_search(self, query: dict, top_k: int=3) -> list:\n",
    "        \"\"\"\n",
    "        Perform a keyword-based search using Elasticsearch.\n",
    "\n",
    "        Args:\n",
    "            query (dict): Elasticsearch query.\n",
    "            top_k (int): Number of top results to return. Defaults to 3.\n",
    "\n",
    "        Returns:\n",
    "            list: Top k search results.\n",
    "        \"\"\"\n",
    "        results = self.es_client.search(index=self.elastic_index_name, body=query)\n",
    "        return [result[\"_source\"] for result in results[\"hits\"][\"hits\"][:top_k]]\n",
    "    \n",
    "    def semantic_search(self, query: str, top_k: int=3) -> list:\n",
    "        \"\"\"\n",
    "        Perform a semantic search using Faiss.\n",
    "\n",
    "        Args:\n",
    "            query (str): Search query.\n",
    "            top_k (int): Number of top results to return. Defaults to 3.\n",
    "\n",
    "        Returns:\n",
    "            list: Top k search results.\n",
    "        \"\"\"\n",
    "        embedding = self.embedding_model.encode([query]).astype('float32')\n",
    "        distances, idx = self.vector_index.search(embedding, top_k)\n",
    "        results = [self.data[i] for i in idx[0]]\n",
    "\n",
    "        return results\n",
    "\n",
    "    def hybrid_search(self, elastic_query: dict, semantic_query: str, top_k: tuple=(3,3), clean_overlap: bool=True) -> list:\n",
    "        \"\"\"\n",
    "        Perform a hybrid search combining results from Elasticsearch and Faiss.\n",
    "\n",
    "        Args:\n",
    "            elastic_query (dict): Elasticsearch query for lexical search.\n",
    "            semantic_query (str): Query string for semantic search.\n",
    "            top_k (tuple): Tuple containing the number of top results to return for (elastic, semantic) searches. Defaults to 3.\n",
    "            clean_overlap (bool): Whether to remove overlap in email threads results. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            list: Combined and deduplicated search results.\n",
    "        \"\"\"\n",
    "        elastic_results = self.elastic_search(elastic_query, top_k[0])\n",
    "        semantic_results = self.semantic_search(semantic_query, top_k[1])\n",
    "\n",
    "        hybrid_concat = pd.concat([pd.DataFrame(elastic_results), pd.DataFrame(semantic_results)], ignore_index=True).drop_duplicates()\n",
    "        hybrid_results = hybrid_concat.to_dict(orient=\"records\")\n",
    "\n",
    "        if clean_overlap:\n",
    "            return self._extract_unique_content(hybrid_results)\n",
    "        return hybrid_results\n",
    "        \n",
    "\n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Remove extra whitespace and newlines from the given text.\n",
    "    \n",
    "        Args:\n",
    "            text (str): The input text to be cleaned.\n",
    "    \n",
    "        Returns:\n",
    "            str: The cleaned text with extra whitespace removed.\n",
    "        \"\"\"\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    def _find_overlap(self, text1: str, text2: str) -> str:\n",
    "        \"\"\"\n",
    "        Find the longest common substring between two texts.\n",
    "    \n",
    "        Args:\n",
    "            text1 (str): The first text to compare.\n",
    "            text2 (str): The second text to compare.\n",
    "    \n",
    "        Returns:\n",
    "            str: The longest common substring, or an empty string if no overlap is found.\n",
    "        \"\"\"\n",
    "        matcher = SequenceMatcher(None, text1, text2)\n",
    "        match = matcher.find_longest_match(0, len(text1), 0, len(text2))\n",
    "        return text1[match.a: match.a + match.size] if match.size > 0 else \"\"\n",
    "\n",
    "    def _extract_unique_content(self, emails: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Extract unique content from a list of email dictionaries by removing overlapping text.\n",
    "        \n",
    "        This function processes a list of email dictionaries, removing any overlapping content\n",
    "        between emails to reduce redundancy. It preserves the original email structure and\n",
    "        metadata while modifying only the 'Mail_Body' field.\n",
    "    \n",
    "        Args:\n",
    "            emails (List[Dict[str, Any]]): A list of dictionaries, each representing an email\n",
    "            keys for 'Origin', 'Subject', 'To', 'From', 'Cc', 'Bcc', 'Date', 'Attachment_Count', \n",
    "            and 'Mail_Body'.\n",
    "    \n",
    "        Returns:\n",
    "            List[Dict[str, Any]]: A list of dictionaries with the same structure as the input,\n",
    "            but with overlapping content removed from the 'Mail_Body' field.\n",
    "    \n",
    "        Note:\n",
    "            This function assumes that emails are ordered chronologically, with newer emails\n",
    "            appearing later in the list.\n",
    "        \"\"\"\n",
    "        unique_contents = []\n",
    "        \n",
    "        for i, email in enumerate(emails):\n",
    "            current_email = self._clean_text(email['Mail_Body'])\n",
    "            unique_content = current_email\n",
    "    \n",
    "            for j in range(i):\n",
    "                previous_email = self._clean_text(emails[j]['Mail_Body'])\n",
    "                overlap = self._find_overlap(previous_email, current_email)\n",
    "                \n",
    "                if len(overlap) > 10:\n",
    "                    unique_content = unique_content.replace(overlap, \"\").strip()\n",
    "    \n",
    "            unique_contents.append({\n",
    "                'Origin': email['Origin'],\n",
    "                'Subject': email['Subject'],\n",
    "                'To': email['To'],\n",
    "                'From': email['From'],\n",
    "                'Cc': email['Cc'],\n",
    "                'Bcc': email['Bcc'],\n",
    "                'Date': email['Date'],\n",
    "                'Attachment_Count': email['Attachment_Count'],\n",
    "                'Mail_Body': unique_content\n",
    "            })\n",
    "    \n",
    "        return unique_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T11:21:09.287614Z",
     "iopub.status.busy": "2024-12-11T11:21:09.287292Z",
     "iopub.status.idle": "2024-12-11T11:24:20.261764Z",
     "shell.execute_reply": "2024-12-11T11:24:20.260989Z",
     "shell.execute_reply.started": "2024-12-11T11:21:09.287587Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"dunzhang/stella_en_1.5B_v5\", trust_remote_code=True, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T11:27:47.235097Z",
     "iopub.status.busy": "2024-12-11T11:27:47.234611Z",
     "iopub.status.idle": "2024-12-11T11:27:47.249232Z",
     "shell.execute_reply": "2024-12-11T11:27:47.248297Z",
     "shell.execute_reply.started": "2024-12-11T11:27:47.235049Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "emails = pd.read_csv(\"/kaggle/input/esights-sample-interface/set1_better.csv\").fillna(\"\") # Index cannot parse nan\n",
    "email_dict = emails.to_dict(orient='records')\n",
    "email_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:02:34.321292Z",
     "iopub.status.busy": "2024-12-11T12:02:34.320621Z",
     "iopub.status.idle": "2024-12-11T12:02:34.369748Z",
     "shell.execute_reply": "2024-12-11T12:02:34.368740Z",
     "shell.execute_reply.started": "2024-12-11T12:02:34.321250Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "search_tool = SemanticHybridSearch(es, model, email_dict, \"/kaggle/input/e-sights-index/index_set_1_elastic.json\", \"/kaggle/input/e-sights-index/index_set_1_semantic.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T11:39:42.235789Z",
     "iopub.status.busy": "2024-12-11T11:39:42.235409Z",
     "iopub.status.idle": "2024-12-11T11:39:42.256807Z",
     "shell.execute_reply": "2024-12-11T11:39:42.255860Z",
     "shell.execute_reply.started": "2024-12-11T11:39:42.235758Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "elastic_search_query = {\n",
    "  \"query\": {\n",
    "    \"bool\": {\n",
    "      \"should\": [\n",
    "        {\n",
    "          \"match\": {\n",
    "            \"To\": {\n",
    "              \"query\": \"Pushpam\",\n",
    "              \"fuzziness\": \"AUTO\"\n",
    "            }\n",
    "          },\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "} # Fuzzy search\n",
    "search_tool.elastic_search(elastic_search_query, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T11:42:06.964585Z",
     "iopub.status.busy": "2024-12-11T11:42:06.963784Z",
     "iopub.status.idle": "2024-12-11T11:42:07.046971Z",
     "shell.execute_reply": "2024-12-11T11:42:07.046055Z",
     "shell.execute_reply.started": "2024-12-11T11:42:06.964543Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "search_tool.semantic_search(\"I want to buy property\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-11T12:02:37.183757Z",
     "iopub.status.busy": "2024-12-11T12:02:37.182803Z",
     "iopub.status.idle": "2024-12-11T12:02:37.294679Z",
     "shell.execute_reply": "2024-12-11T12:02:37.293803Z",
     "shell.execute_reply.started": "2024-12-11T12:02:37.183718Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "search_tool.hybrid_search(elastic_search_query, \"I want to buy property\", (3,3))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6279415,
     "sourceId": 10168135,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6280171,
     "sourceId": 10169135,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
