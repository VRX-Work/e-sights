{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import faiss\n",
    "import json\n",
    "import pandas as pd\n",
    "from elasticsearch.helpers import bulk\n",
    "from difflib import SequenceMatcher\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "class SemanticHybridSearch:\n",
    "    \"\"\"\n",
    "    A class that combines Elasticsearch for keyword-based lexical searches and Faiss for semantic searches.\n",
    "\n",
    "    This class provides methods to load and search both Elasticsearch and Faiss indices,\n",
    "    as well as a hybrid search method that combines results from both search types.\n",
    "\n",
    "    Attributes:\n",
    "        data (list): The dataset used for searching.\n",
    "        es_client (Elasticsearch): Elasticsearch client for performing lexical searches.\n",
    "        embedding_model: Model used for encoding queries into embeddings.\n",
    "        vector_index (faiss.Index): Faiss index for semantic searches.\n",
    "        elastic_index_name (str): Name of the Elasticsearch index.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        es_client,\n",
    "        embedding_model,\n",
    "        data: list,\n",
    "        elastic_index_path: str,\n",
    "        vector_index_path: str,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the SemanticHybridSearch class.\n",
    "\n",
    "        Args:\n",
    "            es_client (Elasticsearch): Elasticsearch client.\n",
    "            embedding_model: Model for encoding queries into embeddings.\n",
    "            data (list): Dataset used for searching.\n",
    "            elastic_index_path (str): Path to the Elasticsearch index file.\n",
    "            vector_index_path (str): Path to the Faiss vector index file.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.es_client = es_client\n",
    "        self.embedding_model = embedding_model\n",
    "        self.vector_index = self.load_vector_index(vector_index_path)\n",
    "        self.elastic_index = self.load_elastic_index(elastic_index_path)\n",
    "\n",
    "        self.elastic_index_name = \"\"\n",
    "\n",
    "    def load_elastic_index(self, elastic_index_path: str):\n",
    "        \"\"\"\n",
    "        Load the Elasticsearch index from a file.\n",
    "\n",
    "        Args:\n",
    "            elastic_index_path (str): Path to the Elasticsearch index file.\n",
    "        \"\"\"\n",
    "        with open(elastic_index_path) as f:\n",
    "            documents = json.load(f)\n",
    "            self.elastic_index_name = os.path.basename(elastic_index_path)\n",
    "            print(f\"Loading Index {self.elastic_index_name}\")\n",
    "\n",
    "            actions = [\n",
    "                {\n",
    "                    \"_index\": self.elastic_index_name,\n",
    "                    \"_id\": doc[\"_id\"],\n",
    "                    \"_source\": doc[\"_source\"],\n",
    "                }\n",
    "                for doc in documents\n",
    "            ]\n",
    "            bulk(self.es_client, actions)\n",
    "\n",
    "    def load_vector_index(self, vector_index_path: str):\n",
    "        \"\"\"\n",
    "        Load the Faiss vector index from a file.\n",
    "\n",
    "        Args:\n",
    "            vector_index_path (str): Path to the Faiss vector index file.\n",
    "\n",
    "        Returns:\n",
    "            faiss.Index: Loaded Faiss index.\n",
    "        \"\"\"\n",
    "        print(f\"Loading Index {os.path.basename(vector_index_path)}\")\n",
    "        index = faiss.read_index(vector_index_path)\n",
    "        return index\n",
    "\n",
    "    def elastic_search(self, query: dict, top_k: int = 3) -> list:\n",
    "        \"\"\"\n",
    "        Perform a keyword-based search using Elasticsearch.\n",
    "\n",
    "        Args:\n",
    "            query (dict): Elasticsearch query.\n",
    "            top_k (int): Number of top results to return. Defaults to 3.\n",
    "\n",
    "        Returns:\n",
    "            list: Top k search results.\n",
    "        \"\"\"\n",
    "        results = self.es_client.search(index=self.elastic_index_name, body=query)\n",
    "        return [result[\"_source\"] for result in results[\"hits\"][\"hits\"][:top_k]]\n",
    "\n",
    "    def semantic_search(self, query: str, top_k: int = 3) -> list:\n",
    "        \"\"\"\n",
    "        Perform a semantic search using Faiss.\n",
    "\n",
    "        Args:\n",
    "            query (str): Search query.\n",
    "            top_k (int): Number of top results to return. Defaults to 3.\n",
    "\n",
    "        Returns:\n",
    "            list: Top k search results.\n",
    "        \"\"\"\n",
    "        embedding = self.embedding_model.encode([query]).astype(\"float32\")\n",
    "        distances, idx = self.vector_index.search(embedding, top_k)\n",
    "        results = [self.data[i] for i in idx[0]]\n",
    "\n",
    "        return results\n",
    "\n",
    "    def hybrid_search(\n",
    "        self,\n",
    "        elastic_query: dict,\n",
    "        semantic_query: str,\n",
    "        top_k: tuple = (3, 3),\n",
    "        clean_overlap: bool = True,\n",
    "    ) -> list:\n",
    "        \"\"\"\n",
    "        Perform a hybrid search combining results from Elasticsearch and Faiss.\n",
    "\n",
    "        Args:\n",
    "            elastic_query (dict): Elasticsearch query for lexical search.\n",
    "            semantic_query (str): Query string for semantic search.\n",
    "            top_k (tuple): Tuple containing the number of top results to return for (elastic, semantic) searches. Defaults to 3.\n",
    "            clean_overlap (bool): Whether to remove overlap in email threads results. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            list: Combined and deduplicated search results.\n",
    "        \"\"\"\n",
    "        elastic_results = self.elastic_search(elastic_query, top_k[0])\n",
    "        semantic_results = self.semantic_search(semantic_query, top_k[1])\n",
    "\n",
    "        hybrid_concat = pd.concat(\n",
    "            [pd.DataFrame(elastic_results), pd.DataFrame(semantic_results)],\n",
    "            ignore_index=True,\n",
    "        ).drop_duplicates()\n",
    "        hybrid_results = hybrid_concat.to_dict(orient=\"records\")\n",
    "\n",
    "        if clean_overlap:\n",
    "            return self._extract_unique_content(hybrid_results)\n",
    "        return hybrid_results\n",
    "\n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Remove extra whitespace and newlines from the given text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to be cleaned.\n",
    "\n",
    "        Returns:\n",
    "            str: The cleaned text with extra whitespace removed.\n",
    "        \"\"\"\n",
    "        return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    def _find_overlap(self, text1: str, text2: str) -> str:\n",
    "        \"\"\"\n",
    "        Find the longest common substring between two texts.\n",
    "\n",
    "        Args:\n",
    "            text1 (str): The first text to compare.\n",
    "            text2 (str): The second text to compare.\n",
    "\n",
    "        Returns:\n",
    "            str: The longest common substring, or an empty string if no overlap is found.\n",
    "        \"\"\"\n",
    "        matcher = SequenceMatcher(None, text1, text2)\n",
    "        match = matcher.find_longest_match(0, len(text1), 0, len(text2))\n",
    "        return text1[match.a : match.a + match.size] if match.size > 0 else \"\"\n",
    "\n",
    "    def _extract_unique_content(\n",
    "        self, emails: List[Dict[str, Any]]\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Extract unique content from a list of email dictionaries by removing overlapping text.\n",
    "\n",
    "        This function processes a list of email dictionaries, removing any overlapping content\n",
    "        between emails to reduce redundancy. It preserves the original email structure and\n",
    "        metadata while modifying only the 'Mail_Body' field.\n",
    "\n",
    "        Args:\n",
    "            emails (List[Dict[str, Any]]): A list of dictionaries, each representing an email\n",
    "            keys for 'Origin', 'Subject', 'To', 'From', 'Cc', 'Bcc', 'Date', 'Attachment_Count',\n",
    "            and 'Mail_Body'.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict[str, Any]]: A list of dictionaries with the same structure as the input,\n",
    "            but with overlapping content removed from the 'Mail_Body' field.\n",
    "\n",
    "        Note:\n",
    "            This function assumes that emails are ordered chronologically, with newer emails\n",
    "            appearing later in the list.\n",
    "        \"\"\"\n",
    "        unique_contents = []\n",
    "\n",
    "        for i, email in enumerate(emails):\n",
    "            current_email = self._clean_text(email[\"Mail_Body\"])\n",
    "            unique_content = current_email\n",
    "\n",
    "            for j in range(i):\n",
    "                previous_email = self._clean_text(emails[j][\"Mail_Body\"])\n",
    "                overlap = self._find_overlap(previous_email, current_email)\n",
    "\n",
    "                if len(overlap) > 10:\n",
    "                    unique_content = unique_content.replace(overlap, \"\").strip()\n",
    "\n",
    "            unique_contents.append(\n",
    "                {\n",
    "                    \"Origin\": email[\"Origin\"],\n",
    "                    \"Subject\": email[\"Subject\"],\n",
    "                    \"To\": email[\"To\"],\n",
    "                    \"From\": email[\"From\"],\n",
    "                    \"Cc\": email[\"Cc\"],\n",
    "                    \"Bcc\": email[\"Bcc\"],\n",
    "                    \"Date\": email[\"Date\"],\n",
    "                    \"Attachment_Count\": email[\"Attachment_Count\"],\n",
    "                    \"Mail_Body\": unique_content,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return unique_contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, List, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolExecutor\n",
    "\n",
    "class State(TypedDict):\n",
    "    accusation: str\n",
    "    queries:Dict[Dict, str]\n",
    "    search_results: List[Dict]\n",
    "    extracted_info: Dict\n",
    "    analysis: Dict\n",
    "    search_count: int\n",
    "\n",
    "class DummyLLM:\n",
    "    def invoke(self, code: str):\n",
    "        pass\n",
    "\n",
    "class InvestigationAgent:\n",
    "    def __init__(self):\n",
    "        self.llm = DummyLLM()\n",
    "        self.search_tool = SemanticHybridSearch()\n",
    "        self.workflow = self._create_workflow()\n",
    "\n",
    "    def _create_workflow(self) -> StateGraph:\n",
    "        workflow = StateGraph(State)\n",
    "        workflow.add_node(\"initial_query\", self.initial_query_generation)\n",
    "        workflow.add_node(\"search\", self.perform_search)\n",
    "        workflow.add_node(\"extract_info\", self.information_extraction)\n",
    "        workflow.add_node(\"analyze\", self.evidence_analysis)\n",
    "        workflow.add_node(\"refine_query\", self.refine_query)\n",
    "\n",
    "        workflow.add_edge(\"initial_query\", \"search\")\n",
    "        workflow.add_edge(\"search\", \"extract_info\")\n",
    "        workflow.add_edge(\"extract_info\", \"analyze\")\n",
    "        workflow.add_conditional_edges(\"analyze\", self.should_continue_search, {\"end\": END, \"refine\": \"refine_query\"})\n",
    "        workflow.add_edge(\"refine_query\", \"search\")\n",
    "        workflow.set_entry_point(\"initial_query\")\n",
    "\n",
    "        return workflow.compile()\n",
    "\n",
    "    def initial_query_generation(self, state: State) -> Dict:\n",
    "        prompt = PromptTemplate.from_template(self._initial_query_prompt())\n",
    "        human_message = HumanMessage(content=prompt.format(accusation=state['accusation']))\n",
    "        ai_message = self.llm.invoke([human_message])\n",
    "        queries = json.loads(ai_message.content)\n",
    "        return {\"queries\": queries}\n",
    "\n",
    "    def perform_search(self, state: Dict) -> Dict:\n",
    "        results = self.search_tool.search(state['queries']['elastic'], state['queries']['semantic'])\n",
    "        return {\"search_results\": results}\n",
    "\n",
    "    def information_extraction(self, state: State) -> Dict:\n",
    "        prompt = PromptTemplate.from_template(self._information_extraction_prompt())\n",
    "        human_message = HumanMessage(content=prompt.format(\n",
    "            accusation=state['accusation'],\n",
    "            results=json.dumps(state['search_results'])\n",
    "        ))\n",
    "        ai_message = self.llm.invoke([human_message])\n",
    "        extracted_info = json.loads(ai_message.content)\n",
    "        return {\"extracted_info\": extracted_info}\n",
    "\n",
    "    def evidence_analysis(self, state: State) -> Dict:\n",
    "        prompt = PromptTemplate.from_template(self._analyze_evidence_prompt())\n",
    "        human_message = HumanMessage(content=prompt.format(\n",
    "            accusation=state['accusation'],\n",
    "            info=json.dumps(state['extracted_info']),\n",
    "            summary=state.get('summary', 'None')\n",
    "        ))\n",
    "        ai_message = self.llm.invoke([human_message])\n",
    "        analysis = json.loads(ai_message.content)\n",
    "        return {\"analysis\": analysis}\n",
    "\n",
    "    def refine_query(self, state: State) -> Dict:\n",
    "        prompt = PromptTemplate.from_template(self._refine_search_prompt())\n",
    "        human_message = HumanMessage(content=prompt.format(\n",
    "            elastic_query=json.dumps(state['queries']['elastic']),\n",
    "            semantic_query=state['queries']['semantic'],\n",
    "            info=json.dumps(state['extracted_info']),\n",
    "            areas=json.dumps(state['analysis']['areas_for_further_investigation']),\n",
    "            accusation=state['accusation']\n",
    "        ))\n",
    "        ai_message = self.llm.invoke([human_message])\n",
    "        refined_queries = json.loads(ai_message.content)\n",
    "        return {\"queries\": refined_queries}\n",
    "\n",
    "    def should_continue_search(self, state: State) -> str:\n",
    "        if state['search_count'] >= 2:\n",
    "            return \"end\"\n",
    "        if state['analysis']['sufficiency']['conclusion'] == \"sufficient\":\n",
    "            return \"end\"\n",
    "        if state['search_count'] > 0 and not self._significant_difference(state['previous_analysis'], state['analysis']):\n",
    "            return \"end\"\n",
    "        return \"refine\"\n",
    "\n",
    "    def _significant_difference(self, prev_analysis: Dict, current_analysis: Dict) -> bool:\n",
    "        # Implement logic to compare previous and current analysis\n",
    "        # Return True if there's a significant difference, False otherwise\n",
    "        pass\n",
    "\n",
    "    def run_investigation(self, accusation: str) -> Dict:\n",
    "        inputs = {\n",
    "            \"accusation\": accusation,\n",
    "            \"search_count\": 0,\n",
    "            \"previous_analysis\": None\n",
    "        }\n",
    "        \n",
    "        for output in self.workflow.stream(inputs):\n",
    "            if \"search_count\" in output:\n",
    "                output[\"search_count\"] += 1\n",
    "            if \"analysis\" in output:\n",
    "                output[\"previous_analysis\"] = output[\"analysis\"]\n",
    "            print(f\"Step: {output['__node__']}\")\n",
    "            print(f\"Output: {json.dumps(output, indent=2)}\")\n",
    "            print(\"---\")\n",
    "        \n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def _initial_query_prompt() -> str:\n",
    "        return \"\"\"Task: Generate initial search queries for the following accusation, suitable for use with the SemanticHybridSearch tool. Accusation: {accusation} Response Format: Provide the response in JSON format with the following keys: elastic: Contains the Elasticsearch query in JSON format. semantic: Contains the semantic search query as a string. Guidelines: Unionized Search Approach: - Combine Elasticsearch and semantic search capabilities effectively. For example: Use Elasticsearch to filter specific fields (e.g., recipients, senders). Use semantic search to refine or specify the context within filtered results. - If only one type of search is required, leave the other key empty (e.g., {} for elastic or \"\" for semantic). Data Schema: { \"Subject\": \"Subject of mail\", \"To\": \"All Recipients\", \"From\": \"Name of sender\", \"Cc\": \"All CC\", \"Bcc\": \"All BCC\", \"Date\": \"Date in datetime format\", \"Attachment_Count\": \"Number of attachments\", \"Mail_Body\": \"Content of the mail in plain text format\" } Elasticsearch Query: - Focus on key terms and concepts relevant to the accusation. - Use appropriate Elasticsearch query DSL structures (e.g., bool, must, should, match, term). - Consider field-specific searches (e.g., subject, body, from, to) and apply boosts where necessary. - Ensure queries are broad enough to capture relevant information but specific enough to exclude irrelevant results. Semantic Search Query: - Use natural language to describe the context and meaning of the accusation. - Incorporate synonyms, related terms, and broader concepts to capture nuances beyond simple keywords. Efficiency and Contextual Relevance: - Adapt search strategies based on the unique aspects of each accusation. - Ensure objectivity and avoid bias in query generation. - Clearly distinguish between facts, inferences, and speculations. Output Example: { \"elastic\": { // Elasticsearch query here }, \"semantic\": \"Semantic search string here\" } Do not provide a preamble or an explanation, the output should strictly be in JSON format with no comments\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _refine_search_prompt() -> str:\n",
    "        return \"\"\"Task: Refine the search queries based on the current queries and extracted information to uncover more details about the accusation. Provide refined queries for both Elasticsearch and semantic search. Current Elasticsearch Query: {elastic_query} Current Semantic Query: {semantic_query} Extracted Info Summary: {info} Areas for Further Investigation: {areas} Accusation: {accusation} Guidelines: Unionized Search Approach: - Combine Elasticsearch and semantic search capabilities effectively. For example: Use Elasticsearch to filter specific fields (e.g., recipients, senders). Use semantic search to refine or specify the context within filtered results. - If only one type of search is required, leave the other key empty (e.g., {} for elastic or \"\" for semantic). Data Schema: { \"Subject\": \"Subject of mail\", \"To\": \"All Recipients\", \"From\": \"Name of sender\", \"Cc\": \"All CC\", \"Bcc\": \"All BCC\", \"Date\": \"Date in datetime format\", \"Attachment_Count\": \"Number of attachments\", \"Mail_Body\": \"Content of the mail in plain text format\" } Your refined queries should: - Build upon the insights gained from the extracted information. - Focus on areas where evidence is lacking or inconclusive. - Include any new relevant terms or concepts discovered in the previous search. - Be more specific than the initial queries, targeting the most promising areas for further investigation. - Utilize Elasticsearch-specific features for the lexical query and natural language for the semantic query. Refined Search Queries: { \"elastic\": { // Elasticsearch query here }, \"semantic\": \"Semantic search string here\" } Do not provide a preamble or an explanation, the output should strictly be in JSON format with no comments\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _information_extraction_prompt() -> str:\n",
    "        return \"\"\"Task: Extract relevant information from the hybrid search results related to the following accusation: Accusation: {accusation} Hybrid Search Results: {results} Analyze the results, which combine Elasticsearch and Faiss search outcomes. Each result contains fields like \"Subject\", \"To\", \"From\", \"Cc\", \"Bcc\", \"Date\", \"Attachment_Count\", and \"Mail_Body\". Provide the following information in JSON format: { \"accused_suspects\": [], \"incident_details\": { \"events\": [ { \"details\": \"\", \"description\": \"\", \"date\": \"\", \"uid\":\"\", } ] }, \"other_parties\": { \"name\": { \"relationship\": \"\", \"role\": \"\", \"uid\":\"uid\", } }, \"summary\": \"\" } Ensure all relevant information is included within this structure. Omit any explanations or additional text outside the JSON.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _analyze_evidence_prompt() -> str:\n",
    "        return \"\"\"Task: Analyze the extracted information and determine if it provides sufficient evidence for the accusation. If not, suggest areas for further investigation. Accusation: {accusation} Extracted Information: {info} Summary of Previous Information: {summary} Provide your analysis in the following JSON format: { \"credibility_and_reliability\": { \"events_analysis\": [ { \"event\": \"Description of the event\", \"credibility_score\": \"Score from 0-100\", \"reasoning\": \"Explanation for the credibility score\", \"uid\": \"The uid of the source where event is mentioned\" } ], \"relationships_analysis\": [ { \"entity1\": \"Name of first entity\", \"entity2\": \"Name of second entity\", \"relationship\": \"Description of relationship\", \"credibility_impact\": \"How this relationship affects credibility\", \"uid\": \"The uid of the source where entities are mentioned\" } ], \"overall_credibility_assessment\": \"Summary of overall credibility\" }, \"sufficiency\": { \"conclusion\": \"One of: sufficient, partial, insufficient\", \"confidence_score\": \"Score from 0-100\", \"conclusion_statement\": \"Detailed explanation of the sufficiency conclusion\", \"refrences\":... [\"List of the uids referenced\"] }, \"areas_for_further_investigation\": [ \"List of specific areas or questions needing further investigation\" ] } Ensure all relevant analysis is included within this structure. Omit any explanations or additional text outside the JSON.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "x = InvestigationAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              +-----------+                \n",
      "              | __start__ |                \n",
      "              +-----------+                \n",
      "                    *                      \n",
      "                    *                      \n",
      "                    *                      \n",
      "            +---------------+              \n",
      "            | initial_query |              \n",
      "            +---------------+              \n",
      "                    *                      \n",
      "                    *                      \n",
      "                    *                      \n",
      "               +--------+                  \n",
      "               | search |                  \n",
      "               +--------+                  \n",
      "             ***         ***               \n",
      "            *               *              \n",
      "          **                 ***           \n",
      "+--------------+                *          \n",
      "| extract_info |                *          \n",
      "+--------------+                *          \n",
      "        *                       *          \n",
      "        *                       *          \n",
      "        *                       *          \n",
      "  +---------+                   *          \n",
      "  | analyze |..                 *          \n",
      "  +---------+  ...              *          \n",
      "        .         .....         *          \n",
      "        .              ...      *          \n",
      "        .                 ...   *          \n",
      "  +---------+           +--------------+   \n",
      "  | __end__ |           | refine_query |   \n",
      "  +---------+           +--------------+   \n"
     ]
    }
   ],
   "source": [
    "print(x.workflow.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intella",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
