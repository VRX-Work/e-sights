{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, List, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolExecutor\n",
    "\n",
    "class State(TypedDict):\n",
    "    accusation: str\n",
    "    queries:Dict[Dict, str]\n",
    "    search_results: List[Dict]\n",
    "    extracted_info: Dict\n",
    "    analysis: Dict\n",
    "    search_count: int\n",
    "\n",
    "\n",
    "class SemanticHybridSearch:\n",
    "    def search(self, elastic_query: Dict, semantic_query: str) -> List[Dict]:\n",
    "        # Placeholder\n",
    "        pass\n",
    "\n",
    "class DummyLLM:\n",
    "    def invoke(self, code: str):\n",
    "        pass\n",
    "\n",
    "class InvestigationAgent:\n",
    "    def __init__(self):\n",
    "        self.llm = DummyLLM()\n",
    "        self.search_tool = SemanticHybridSearch()\n",
    "        self.workflow = self._create_workflow()\n",
    "\n",
    "    def _create_workflow(self) -> StateGraph:\n",
    "        workflow = StateGraph(State)\n",
    "        workflow.add_node(\"initial_query\", self.initial_query_generation)\n",
    "        workflow.add_node(\"search\", self.perform_search)\n",
    "        workflow.add_node(\"extract_info\", self.information_extraction)\n",
    "        workflow.add_node(\"analyze\", self.evidence_analysis)\n",
    "        workflow.add_node(\"refine_query\", self.refine_query)\n",
    "\n",
    "        workflow.add_edge(\"initial_query\", \"search\")\n",
    "        workflow.add_edge(\"search\", \"extract_info\")\n",
    "        workflow.add_edge(\"extract_info\", \"analyze\")\n",
    "        workflow.add_conditional_edges(\"analyze\", self.should_continue_search, {\"end\": END, \"refine\": \"refine_query\"})\n",
    "        workflow.add_edge(\"refine_query\", \"search\")\n",
    "        workflow.set_entry_point(\"initial_query\")\n",
    "\n",
    "        return workflow.compile()\n",
    "\n",
    "    def initial_query_generation(self, state: State) -> Dict:\n",
    "        prompt = PromptTemplate.from_template(self._initial_query_prompt())\n",
    "        human_message = HumanMessage(content=prompt.format(accusation=state['accusation']))\n",
    "        ai_message = self.llm.invoke([human_message])\n",
    "        queries = json.loads(ai_message.content)\n",
    "        return {\"queries\": queries}\n",
    "\n",
    "    def perform_search(self, state: Dict) -> Dict:\n",
    "        results = self.search_tool.search(state['queries']['elastic'], state['queries']['semantic'])\n",
    "        return {\"search_results\": results}\n",
    "\n",
    "    def information_extraction(self, state: State) -> Dict:\n",
    "        prompt = PromptTemplate.from_template(self._information_extraction_prompt())\n",
    "        human_message = HumanMessage(content=prompt.format(\n",
    "            accusation=state['accusation'],\n",
    "            results=json.dumps(state['search_results'])\n",
    "        ))\n",
    "        ai_message = self.llm.invoke([human_message])\n",
    "        extracted_info = json.loads(ai_message.content)\n",
    "        return {\"extracted_info\": extracted_info}\n",
    "\n",
    "    def evidence_analysis(self, state: State) -> Dict:\n",
    "        prompt = PromptTemplate.from_template(self._analyze_evidence_prompt())\n",
    "        human_message = HumanMessage(content=prompt.format(\n",
    "            accusation=state['accusation'],\n",
    "            info=json.dumps(state['extracted_info']),\n",
    "            summary=state.get('summary', 'None')\n",
    "        ))\n",
    "        ai_message = self.llm.invoke([human_message])\n",
    "        analysis = json.loads(ai_message.content)\n",
    "        return {\"analysis\": analysis}\n",
    "\n",
    "    def refine_query(self, state: State) -> Dict:\n",
    "        prompt = PromptTemplate.from_template(self._refine_search_prompt())\n",
    "        human_message = HumanMessage(content=prompt.format(\n",
    "            elastic_query=json.dumps(state['queries']['elastic']),\n",
    "            semantic_query=state['queries']['semantic'],\n",
    "            info=json.dumps(state['extracted_info']),\n",
    "            areas=json.dumps(state['analysis']['areas_for_further_investigation']),\n",
    "            accusation=state['accusation']\n",
    "        ))\n",
    "        ai_message = self.llm.invoke([human_message])\n",
    "        refined_queries = json.loads(ai_message.content)\n",
    "        return {\"queries\": refined_queries}\n",
    "\n",
    "    def should_continue_search(self, state: State) -> str:\n",
    "        if state['search_count'] >= 2:\n",
    "            return \"end\"\n",
    "        if state['analysis']['sufficiency']['conclusion'] == \"sufficient\":\n",
    "            return \"end\"\n",
    "        if state['search_count'] > 0 and not self._significant_difference(state['previous_analysis'], state['analysis']):\n",
    "            return \"end\"\n",
    "        return \"refine\"\n",
    "\n",
    "    def _significant_difference(self, prev_analysis: Dict, current_analysis: Dict) -> bool:\n",
    "        # Implement logic to compare previous and current analysis\n",
    "        # Return True if there's a significant difference, False otherwise\n",
    "        pass\n",
    "\n",
    "    def run_investigation(self, accusation: str) -> Dict:\n",
    "        inputs = {\n",
    "            \"accusation\": accusation,\n",
    "            \"search_count\": 0,\n",
    "            \"previous_analysis\": None\n",
    "        }\n",
    "        \n",
    "        for output in self.workflow.stream(inputs):\n",
    "            if \"search_count\" in output:\n",
    "                output[\"search_count\"] += 1\n",
    "            if \"analysis\" in output:\n",
    "                output[\"previous_analysis\"] = output[\"analysis\"]\n",
    "            print(f\"Step: {output['__node__']}\")\n",
    "            print(f\"Output: {json.dumps(output, indent=2)}\")\n",
    "            print(\"---\")\n",
    "        \n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def _initial_query_prompt() -> str:\n",
    "        return \"\"\"Task: Generate initial search queries for the following accusation, suitable for use with the SemanticHybridSearch tool. Accusation: {accusation} Response Format: Provide the response in JSON format with the following keys: elastic: Contains the Elasticsearch query in JSON format. semantic: Contains the semantic search query as a string. Guidelines: Unionized Search Approach: - Combine Elasticsearch and semantic search capabilities effectively. For example: Use Elasticsearch to filter specific fields (e.g., recipients, senders). Use semantic search to refine or specify the context within filtered results. - If only one type of search is required, leave the other key empty (e.g., {} for elastic or \"\" for semantic). Data Schema: { \"Subject\": \"Subject of mail\", \"To\": \"All Recipients\", \"From\": \"Name of sender\", \"Cc\": \"All CC\", \"Bcc\": \"All BCC\", \"Date\": \"Date in datetime format\", \"Attachment_Count\": \"Number of attachments\", \"Mail_Body\": \"Content of the mail in plain text format\" } Elasticsearch Query: - Focus on key terms and concepts relevant to the accusation. - Use appropriate Elasticsearch query DSL structures (e.g., bool, must, should, match, term). - Consider field-specific searches (e.g., subject, body, from, to) and apply boosts where necessary. - Ensure queries are broad enough to capture relevant information but specific enough to exclude irrelevant results. Semantic Search Query: - Use natural language to describe the context and meaning of the accusation. - Incorporate synonyms, related terms, and broader concepts to capture nuances beyond simple keywords. Efficiency and Contextual Relevance: - Adapt search strategies based on the unique aspects of each accusation. - Ensure objectivity and avoid bias in query generation. - Clearly distinguish between facts, inferences, and speculations. Output Example: { \"elastic\": { // Elasticsearch query here }, \"semantic\": \"Semantic search string here\" } Do not provide a preamble or an explanation, the output should strictly be in JSON format with no comments\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _refine_search_prompt() -> str:\n",
    "        return \"\"\"Task: Refine the search queries based on the current queries and extracted information to uncover more details about the accusation. Provide refined queries for both Elasticsearch and semantic search. Current Elasticsearch Query: {elastic_query} Current Semantic Query: {semantic_query} Extracted Info Summary: {info} Areas for Further Investigation: {areas} Accusation: {accusation} Guidelines: Unionized Search Approach: - Combine Elasticsearch and semantic search capabilities effectively. For example: Use Elasticsearch to filter specific fields (e.g., recipients, senders). Use semantic search to refine or specify the context within filtered results. - If only one type of search is required, leave the other key empty (e.g., {} for elastic or \"\" for semantic). Data Schema: { \"Subject\": \"Subject of mail\", \"To\": \"All Recipients\", \"From\": \"Name of sender\", \"Cc\": \"All CC\", \"Bcc\": \"All BCC\", \"Date\": \"Date in datetime format\", \"Attachment_Count\": \"Number of attachments\", \"Mail_Body\": \"Content of the mail in plain text format\" } Your refined queries should: - Build upon the insights gained from the extracted information. - Focus on areas where evidence is lacking or inconclusive. - Include any new relevant terms or concepts discovered in the previous search. - Be more specific than the initial queries, targeting the most promising areas for further investigation. - Utilize Elasticsearch-specific features for the lexical query and natural language for the semantic query. Refined Search Queries: { \"elastic\": { // Elasticsearch query here }, \"semantic\": \"Semantic search string here\" } Do not provide a preamble or an explanation, the output should strictly be in JSON format with no comments\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _information_extraction_prompt() -> str:\n",
    "        return \"\"\"Task: Extract relevant information from the hybrid search results related to the following accusation: Accusation: {accusation} Hybrid Search Results: {results} Analyze the results, which combine Elasticsearch and Faiss search outcomes. Each result contains fields like \"Subject\", \"To\", \"From\", \"Cc\", \"Bcc\", \"Date\", \"Attachment_Count\", and \"Mail_Body\". Provide the following information in JSON format: { \"accused_suspects\": [], \"incident_details\": { \"events\": [ { \"details\": \"\", \"description\": \"\", \"date\": \"\", \"uid\":\"\", } ] }, \"other_parties\": { \"name\": { \"relationship\": \"\", \"role\": \"\", \"uid\":\"uid\", } }, \"summary\": \"\" } Ensure all relevant information is included within this structure. Omit any explanations or additional text outside the JSON.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _analyze_evidence_prompt() -> str:\n",
    "        return \"\"\"Task: Analyze the extracted information and determine if it provides sufficient evidence for the accusation. If not, suggest areas for further investigation. Accusation: {accusation} Extracted Information: {info} Summary of Previous Information: {summary} Provide your analysis in the following JSON format: { \"credibility_and_reliability\": { \"events_analysis\": [ { \"event\": \"Description of the event\", \"credibility_score\": \"Score from 0-100\", \"reasoning\": \"Explanation for the credibility score\", \"uid\": \"The uid of the source where event is mentioned\" } ], \"relationships_analysis\": [ { \"entity1\": \"Name of first entity\", \"entity2\": \"Name of second entity\", \"relationship\": \"Description of relationship\", \"credibility_impact\": \"How this relationship affects credibility\", \"uid\": \"The uid of the source where entities are mentioned\" } ], \"overall_credibility_assessment\": \"Summary of overall credibility\" }, \"sufficiency\": { \"conclusion\": \"One of: sufficient, partial, insufficient\", \"confidence_score\": \"Score from 0-100\", \"conclusion_statement\": \"Detailed explanation of the sufficiency conclusion\", \"refrences\":... [\"List of the uids referenced\"] }, \"areas_for_further_investigation\": [ \"List of specific areas or questions needing further investigation\" ] } Ensure all relevant analysis is included within this structure. Omit any explanations or additional text outside the JSON.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "x = InvestigationAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              +-----------+                \n",
      "              | __start__ |                \n",
      "              +-----------+                \n",
      "                    *                      \n",
      "                    *                      \n",
      "                    *                      \n",
      "            +---------------+              \n",
      "            | initial_query |              \n",
      "            +---------------+              \n",
      "                    *                      \n",
      "                    *                      \n",
      "                    *                      \n",
      "               +--------+                  \n",
      "               | search |                  \n",
      "               +--------+                  \n",
      "             ***         ***               \n",
      "            *               *              \n",
      "          **                 ***           \n",
      "+--------------+                *          \n",
      "| extract_info |                *          \n",
      "+--------------+                *          \n",
      "        *                       *          \n",
      "        *                       *          \n",
      "        *                       *          \n",
      "  +---------+                   *          \n",
      "  | analyze |..                 *          \n",
      "  +---------+  ...              *          \n",
      "        .         .....         *          \n",
      "        .              ...      *          \n",
      "        .                 ...   *          \n",
      "  +---------+           +--------------+   \n",
      "  | __end__ |           | refine_query |   \n",
      "  +---------+           +--------------+   \n"
     ]
    }
   ],
   "source": [
    "print(x.workflow.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intella",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
